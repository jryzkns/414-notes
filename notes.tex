\documentclass{article}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry} % lots more margin
\pagenumbering{gobble} % ignore page numbers

\usepackage{titling}
\setlength{\droptitle}{-0.75in}

\title{CMPT 414 review notes}
\author{}
\date{}

\setlength{\parindent}{0cm}

\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref} % for nice looking urls
\usepackage{booktabs} % for making tables
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{multicol}
\usepackage{float}

\begin{document}

\maketitle

\begin{multicols}{2}

\section{Image Processing}

In this section we outline methods that can be used to modify images. For our purposes we only concern ourselves with grayscale images. For color images, we can apply the same methods after transforming the color space into one that has separatable luminance and chrominance components (ex. YCbCr).

\subsection{Histogram Transformations}

An image histogram is constructed by the frequencies of pixel intensities. By modifying pixel intensities on a histogram-level we can alter and often improve the quality of images. The transformations are applied to the image histogram, but changes in histogram values reflect the mapping of pixel values in the image. We outline two methods below:

\subsubsection{Linear Stretching}

Sometimes the image histogram may only be occupying part of the intensity range (ex. mostly dark pixels or mostly light pixels). As a result, the image would have bad contrast.

One way to address this issue is to stretch the histogram so that it occupies the entire intensity range. To do this, we mark the top and bottom of the histogram (\texttt{a} and \texttt{b}, respectively) and for each pixel in the histogram, compute a proportion of where it stands between \texttt{a} and \texttt{b}. Based on these proportions, the histogram is then mapped to occupy the entire intensity range.

The algorithm is as follows. \texttt{p\_0} and \texttt{p\_m} are the minimum and maximum pixel intensities respectively.

\begin{verbatim}
function linear_stretch (img, a, b, p_0, p_m)
  for pix in img:
    if     pix <= a: pix = p_0
    elseif pix >= b: pix = p_m
    else:  pix = (p_m-p_0)/(b-a)*(pix-a)+p_0
end
\end{verbatim}

One quick note is that we can set \texttt{a} and \texttt{b} to not be the max and min of the histogram, which then makes some pixels clip to the min or max.

\subsubsection{Histogram Equalization}

Histogram Equalization is another popular method that is often used for improving the constrast of the messages. The idea of Histogram Equalization is to transform the intensity histogram to be a uniform histogram. In practice, the resulting histogram will not be perfectly uniform. 

In essence: by spacing out pixel intensities, regions in the image where there are lots of detail but only represented with a few intensities become easier to process.

The algorithm is as follows: \texttt{hist} is the original histogram of the image, and \texttt{n\_factor} is the ratio between amount of pixels in the image and the levels of pixel intensities.

\begin{verbatim}
function hist_eq (img, hist, n_factor)
  for pix in img:
    pix = ceil(sum(hist[:p])/n_factor) - 1
end
\end{verbatim}

\subsection{Convolutions and filtering} 

A convolution is a linear and shift-invariant operator. Shift invariance implies that if $y(n)$ is the response to $x(n)$, then $y(n-k)$ is the response to $x(n-k)$. Linearity is defined in the usual manner. For our purposes, a 2D convolution is equivalent to filtering an image, where a filter is defined by a matrix that we call a mask.

For an image $f$ and a mask $t$, a 2D convolution is defined as the following:

\[
\begin{aligned}
f * t &= \iint_{-\infty}^{\infty} f(x-x', y-y') \cdot t(x',y') dx'dy'\\
      &= \sum_{x' = -M}^{M}\sum_{y' = -N}^{N} f(x-x', y-y') \cdot t(x', y')
\end{aligned}    
\]

For the discrete case, the mask has to be in dimensions of $(2M+1) \times (2N + 1)$.

\subsubsection{Cross-Correlations}

A cross-correlation between $f$ and $t$ is defined as 
$$
R_{ft}(x, y) = \sum_{x' = -M}^M \sum_{y' = -N}^N f(x+x', y+y') t(x',y')
$$

The above definition is not invariant to the magnitude of $f$. A normalized $R_{ft}$ is needed:

$$
R_{ft} = \frac{\sum_{x', y'}f(x+x', y+y') t(x',y')}{\sqrt{\sum_{x',y'}f(x+x', y+y')^2 \cdot \sum_{x',y'}t(x',y')^2}}
$$

\subsubsection{Mask Design}

Masks are usually matrices whose values are designed in a specific way to accomodate different uses. 

In computer vision, we usually use masks that are symmetric. This implies that we do not need to concern ourselves with demarcating convolutions and cross-correlations.

When the same mask is applied twice (for example, applied onto itself), we will see that the mask grows in size. For example:

$$
( \;1\; 1\; 1\; ) * ( \;1\; 1\; 1\; ) = ( \;1 \;2 \;3 \;2 \;1 \;)
$$

It is implied that mask values are normalized, so the previous example should really be

$$
\left( \;\frac{1}{3}\; \frac{1}{3}\; \frac{1}{3}\; \right) * \left( \;\frac{1}{3}\; \frac{1}{3}\; \frac{1}{3}\; \right) = \left( \;\frac{1}{9} \;\frac{2}{9} \;\frac{3}{9} \;\frac{2}{9} \;\frac{1}{9} \;\right)
$$

For a target 2D mask, it may be worthwhile to decompose into a kronecker product of two 1D masks, For example:

$$
\left(\begin{matrix} 1& 0 & -1 \end{matrix}\right) \otimes \left(\begin{matrix} 1\\ 2\\ 1 \end{matrix}\right) = \left(\begin{matrix} 1 & 0 & -1\\ 2 & 0 & -2\\ 1 & 0 & -1 \end{matrix}\right)
$$

A better visualization is the following, each entry in the matrix is the product of the row value and column value:
\begin{figure}[H]
  \centering
\begin{tabular}{c|ccc}
  & \textbf{1} & \textbf{0} & \textbf{-1}\\
  \hline
  \textbf{1} & 1 & 0 & -1\\
  \textbf{2} & 2 & 0 & -2\\
  \textbf{1} & 1 & 0 & -1
\end{tabular}
\end{figure}

The merit in decomposing the mask is to do with the time complexity of applying a filter. Each entry in the mask is used in a multiplication once for each pixel in the image (suppose the image is $N \times N$). Thus for a 2D $k \times k$ mask the time complexity will be $O(k^2N^2)$. However, for two 1D $k \times 1$ masks the time complexity will only be $O(kN^2)$.

\subsubsection{Frequency Analysis}

Images in the $(x,y)$ domain can be decomposed into sums of spatial frequencies in frequency domain via an integral transform mechanism (ex. DCT, ADST, etc). Spatial frequencies can be categorized into high frequencies or low frequencies. Applying a filter that attenuates (reduces) high frequencies while passing low frequencies is called a low-pass filter. The opposite is called a high-pass filter. The demarcation for a filter to distinguish between low and high frequency is called a cut-off frequency.

Suppose we have an integral transform $\mathcal{I}$, there are important properties to exploit. In the foremost, a convolution in image space is translated to a multiplication in frequency space:

$$
\mathcal{I}(f * t) = \mathcal{I}(f) \times \mathcal{I}(t)
$$

\subsection{Smoothing}

Smoothing is a low-pass filtering operation. It is also called de-noising. While smoothing will remove noise, it will also remove the sharpness of the image, resulting in a blurred effect. In a sense, de-noising the image is achieved by smearing the image.

\subsubsection{Unweighted Averaging}

One naive approach to blur an image is to use a $k \times k$ mask that with all the same entries (i.e. all 1's; keep in mind that normalization is implied). For example, such a $4 \times 4$ mask would look like this:

$$
\left[\begin{matrix} 1&1&1&1\\1&1&1&1\\1&1&1&1\\1&1&1&1\\\end{matrix}\right]
$$

\subsubsection{K-nearest Neighbor Averaging}

This method involves investigating a $m \times m$ neighborhood around each image pixel. $K$ neighbors that are nearest to the centre image pixel value are taken and their average is preserved. This method is edge-preserving, but non-linear.

\subsubsection{Median Filtering}

This method also involves investigating a $m \times m$ neighborhood, but instead the median of the neighborhood is taken. This method is also very effective against salt and pepper (S\&P) noise.

This method is also edge-preserving and non-linear, and additionally it does not preserve corners. One way to mitigate this issue is to choose a different set of values to observe in the neighborhood. For example, in a cross shape instead of the entire neighborhood.

\subsubsection{Gaussian Filtering}

Gaussian filtering is a popular method for denoising images, it is also used as a primitive in many other applications in computer vision. This is because the operation resembles optical blur, and a Gaussian mask has good mathematical properties.

The filter is parametrized with two parameters $(\sigma^2_x,\sigma^2_y)$, but we will treat the filter as if it has only one parameter $\sigma^2 = \sigma^2_x = \sigma^2_y$.

One important mathematical property of a Gaussian is that the integral transform of a Gaussian is still a Gaussian. For a Gaussian function $G(x)$ with variance $\sigma^2$, the integral transform response of the Gaussian $\mathcal{I}(G(x))$ has variance $\sigma_f^2$ where the relationship between $\sigma$ and $\sigma_f$ is described as $\sigma \cdot \sigma_f = (2\pi)^{-1}$. As such, the larger $\sigma$ is, the lower $\sigma_f$ is, which gives a lower cut-off frequency.

\subsection{Sharpening}

Sharpening is a high-pass filtering operation. It is the opposite of smoothing and as such it is sometimes called de-blurring.

\subsubsection{Laplacian-based Methods}

The Laplacian (2nd derivative) sharpening operator is defined as $f - \nabla \cdot \nabla f$. The Laplacian $\nabla \cdot \nabla f = \frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2}$ is derived as follows:

\[
\begin{aligned}
  \frac{\partial^2f}{\partial x^2} &= f(x+1,y) - f(x,y) - (f(x,y) - f(x-1,y))\\
  & = f(x+1,y) + f(x-1,y) - 2f(x,y) \\ 
  \frac{\partial^2f}{\partial y^2} &= f(x,y+1) - f(x,y) - (f(x,y) - f(x,y-1))\\
  & = f(x,y+1) + f(x,y-1) - 2f(x,y)
\end{aligned}
\]

As such, $\nabla^2 f(x,y) = f(x+1,y) + f(x-1,y) + f(x,y+1) + f(x,y-1) - 4f(x,y)$, which is similar to a blurred version of the image. The mechanism that a laplacian operator operates on is simply to subtract the noise from the image by subtracting a noisy version of the image: $\hat f(x,y) = 5f(x,y) - [f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)]$. This corresponds nicely to the following mask:

$$
\begin{bmatrix}
  0 & -1 & 0\\
  -1 & 5 & -1\\
  0 & -1 & 0
\end{bmatrix}
$$

A common variation on this operator is to give weight to the laplacian, so the operator is defined as $f - w \nabla \cdot \nabla f$. While this method is effective at sharpening the image, it also creates lots of artefacts and noise.

\subsubsection{Unsharp Masking}

In a similar spirit to the laplacian operator, unsharp masking (USM) operates by adding a scaled and inverted version(inversion is the same as subtraction) of the blurred image to the original. The blurring mechanism is to simulate the noise model using a blurring mechanism in the image instead of using a derivative based method. USM generally performs better than the Laplacian operator, but it will take more time to run as it is more than just one simple filter.

\section{Image Moments}

For an image $f(x,y)$, the $(p,q)$\textsuperscript{th} order moment is defined as the following:

\[
\begin{aligned}
  m(p,q) &= \iint x^p y^q f(x,y) dxdy\\
         &= \sum_x \sum_y x^p y^q f(x,y)
\end{aligned}  
\]

Suppose we have a segmented image represented by a binary image $B$, image moments can tell us different statistics about the image.

$m(0,0)$ for the image tells us the amount of pixels that are 1 in $B$, or the area of 1's:

$$
A = \sum_i \sum_j B[i,j]
$$

The first moment in either axis helps us identify the centroid of 1's:

$$
\bar x = \frac{1}{A}\sum_i \sum_j j \cdot B[i,j] \;\;\; \bar y = \frac{1}{A} \sum_i \sum_j i \cdot B[i,j]
$$

The axis of the least 2nd moment is the axis of elongation (orientation) for 2D regions.

\section{Template Matching}

Similarity between $M \times N$ image $f$ and a $k \times s$ template $t$ can be measured by their Euclidean distance, but the sum of squares itself will do:

\[
\begin{aligned}
  &d^2(x,y)\\
  =&\sum_{k,s}[f(x+k,y+s) - t(k,s)]^2 \\
  =&\sum_{k,s}[f(x+k,y+s)^2 -2f(x+k,y+s)t(k,s) + t(k,s)^2]
\end{aligned}
\]

If a perfect match is found at $(x',y')$, then $d^2(x',y') = 0$.

Our goal is to minimize $d^2(x,y)$. $f(x+k,y+s)^2$ is independent from t, and $t(k,s)^2$ is a constant. Therefore we only need to focus on maximizing $\sum_{k,s}f(x+k,y+s)t(k,s)$ to minimize $d^2(x,y)$, which is in effect looking for minimas in a cross-correlation.

\section{Edge Detection}

We define edges as intensity discontinuities in images. Using edge information is useful for many computer vision applications. We will discuss many methods of edge detection in this section.

\subsection{Gradient-Based Methods}

In this category of methods, the image gradient is computed in one way or another to derive edge information.

\[
\begin{aligned}
  \frac{\partial f}{\partial x} = f(x+n, y) - f(x,y) \;& \;\frac{\partial f}{\partial y} = f(x, y+n) - f(x,y)\\
  S = \sqrt{\left(\frac{\partial f}{\partial x}\right)^2 + \left(\frac{\partial f}{\partial y}\right)^2} \;&\; \theta = \arctan\left(\frac{\partial f}{\partial x} \middle/ \frac{\partial f}{\partial y}\right)
\end{aligned}
\]

Usually, $n=1$ for computing the partial derivatives. In this context, $S$ is the gradient magnitude, and $\theta$ is the directional information of the gradient. All of these are computed over the entire image, so there's a map of all of these values corresponding to each pixel.

\subsubsection{Gradient Thresholding}

Simply put, if the magnitude $S$ is greater than a threshold $t$, then it is an edge pixel, and an edge map can be generated from it.

\subsubsection{Roberts Operator}

The roberts operator computes $S$ and $\theta$ differently. Instead of using $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$, it generates two filtered images with the following two masks:

$$
G_x = \left[\begin{matrix}1 & 0\\ 0 & -1\end{matrix}\right] \quad\quad G_y = \left[\begin{matrix}0 & 1\\ -1 & 0\end{matrix}\right]
$$

$S$ and $\theta$ are derived by the following:

$$
S = \sqrt{\left(G_x * f\right)^2 + \left(G_y * f\right)^2} \;\; \theta = \arctan\left(\frac{G_y * f}{G_x * f}\right) - \frac{3\pi}{4}
$$

The rest follows a simple thresholding procedure.

\subsubsection{Prewitt and Sobel Operators}

Prewitt uses 8 operators for each of the 8 ordinal-cardinal directions. The $0^{\circ}$ and $45^{\circ}$ operators are as follows:

$$
\begin{bmatrix}
  -1 & 0 & 1\\
  -1 & 0 & 1\\
  -1 & 0 & 1
\end{bmatrix}\quad
\begin{bmatrix}
  0 & 1 & 1\\
  -1 & 0 & 1\\
  -1 & -1 & 0
\end{bmatrix}
$$

This method includes providing directional information and uses local averages to reduce noise. However, the edge width can be greater than 1, which means there isn't a single output.

The Sobel operator is similar to Prewitt's, except it places a stronger emphasis on the weighting of the direction each mask is meant for. Below are the corresponding masks to compare with their Prewitt counterparts:

$$
\begin{bmatrix}
  -1 & 0 & 1\\
  -2 & 0 & 2\\
  -1 & 0 & 1
\end{bmatrix}\quad
\begin{bmatrix}
  0 & 1 & 2\\
  -1 & 0 & 1\\
  -2 & -1 & 0
\end{bmatrix}
$$

The $0^{\circ}$ and $90^{\circ}$ Sobel operators are often used to generate gradient information.

\subsection{Laplacian-Based Methods}

Instead of working with the first derivative, we can work with the second derivative (which is a laplacian) to derive edge information.

\subsubsection{Zero Crossings}

The laplacian edge operator we describe here is different from the sharpening operator. Namely, the laplacian edge operator is 

$$
\begin{bmatrix}
  0 & -1 & 0\\
  -1 & 4 & -1\\
  0 & -1 & 0
\end{bmatrix}
$$

Whereas the laplacian sharpening operator has a midle entry of 5 instead of 4.

After applying the operator, we look for zero-crossings in the resulting image to determine where the edges are. Zero crossings are simply where there is a negative value and a neighboring positive value.

\subsubsection{Marr's Operator}

Marr's Operator applies a gaussian blur $G$ to the image $f$ before calculating the laplacian to find zero crossings:

$$
\nabla^2 (G(x,y) * f(x,y))
$$

Due to a property for convolutions, the above is equivalent to 

$$
(\nabla^2 G(x,y)) * f(x,y)
$$

Thus operator is also called a laplacian of gaussian (LOG) operator. The gaussian's $\sigma$ can be tuned for different spatial frequencies.

Additionally, a laplacian of gaussian can be approximated by the difference between two gaussians, called a difference of gaussians (DOG).

\subsection{Marr's Edge Operator}

Given an image f(x,y), edges can be found by locating the zero crossings from \(D^2(G(x,y)*f(x,y))\), where G is a Gaussian function and $D^2$ is the second derivative.
Derivative rule for convolution: \(D^2(G*f) = D^2G * f\).

\subsection{Canny Edge Operator}

After discussing previous edge detection operators, we collect some thoughts to approach another edge operator.

\subsubsection{Criteria for Good Edge Detection}
\begin{itemize}
      \item Always find real edges, not false ones
  
      \item Good localization, where found edges are as close to the original as possible
  
      \item Unique Output
\end{itemize}

\subsubsection{Stages in Canny Edge Detection}

\begin{enumerate}
\item \textbf{Smoothing}

Apply Gaussian Blur to the image $I' = G*I$.

\item \textbf{Gradient Operator}

Use the $0^{\circ}$ and $90^{\circ}$ Sobel operators to generate $\frac{\partial I'}{\partial x}$ and $\frac{\partial I'}{\partial y}$. Using these, generate $S$ and $theta$.

\item \textbf{Non-maximum Suppression}

Non-maximum suppression thins out the edges that result from the gradient operator.

For each pixel $p$ in $S$, examine two neighbors along the direction of $\theta$. The pixel will be suppressed (set to 0) if $p$ is not the largest value among its neighbors, and the directions of its neighbors don't differ from $p$ by more than $45^{\circ}$.

\item \textbf{Double Thresholding}

Double thresholding determines the strength of each pixel, whether it's weak, strong, or irrelevant.

There are two thresholds, a high threshold $Th_h$ to tell apart strong and weak edge pixels. A low threshold $Th_s$ is then used to tell apart weak and irrelevant pixels. Strong pixels are given a value, and weak pixels are given another value. The resulting matrix will only have three unique values.

\item \textbf{Hysteresis Tracking}

This is used to determine if weak edge pixels are going to be transformed into strong edge pixels and included in the final edge map. Otherwise they are discarded.

If a weak edge pixel has at least one strong edge pixel surrounding it, then it is turned into a strong pixel.

The final edge map is defined as the map of all the strong edge pixels.

\end{enumerate}

\section{Regions and Segmentation}

A region is a group of connected pixels with similar properties in an image.

\subsection{Image Segmentation}

Image segmentation is a process to divide an image into regions. It can be done in a manner that is region-based, edge-based, or in a hybrid approach.

Some of the methods for Image Segmentation are as follows
\begin{itemize}
  \item {Thresholding}
  \item {Blob colouring (for binary images)}
  \item {Split and Merge}
  \item {K-Means}
  \item {Mean Shift}
\end{itemize}

\subsection{Gestalt Psychology and Human Perception}

Grouping is the key to visual perception. The Law of Pragnanz in Gestalt psychology states that people perceive ambiguous or complex images as the simplest form possible.

Here are some of the heuristics that can be used to group items together.
\begin{multicols}{3}
\begin{itemize}
  \item {Proximity}
  \item {Similarity}
  \item {Common Fate}
  \item {Common Region}
  \item {Parallelism}
  \item {Symmetry}
  \item {Continuity}
  \item {Closure}
  \item {Familiar Shapes}
\end{itemize}
\end{multicols}

\section{Texture Analysis}

Textures are defined as repeated patterns of local intensity variations, and it depends on the image resolution.
There are three issues related to texture analysis:

There are three main tasks in texture analysis:
\begin{itemize}
  \item {texture classification}
  \item {texture segmentation}
  \item {shape from texture}
\end{itemize}

\subsection{Statistical Methods}

\subsubsection{Spatial Gray Level Dependence Method}

The spatial gray level dependence method utilizes a gray level co-occurence matrix $P$. It is a $M \times M$ where $M$ is the amount of gray levels. $P$ serves as a tally of pixel-value pairs that follow a predefined pixel structure $\mathbf{d}$. Due to the particularity of $\mathbf{d}$, $P$ is not symmetric. $P$ is also normalized, and as such treated as a pmf.

$\mathbf{d}$ is defined as a pair of integral values $(a,b)$ which is to be interpreted as "starting with pixel intensity $i$, move $a$ units to the right then $b$ units down to get to pixel intesity $j$". In other words, a valid pixel pair satisfying $\mathbf{d}$ is $f(x,y)=i$ and $f(x+a,y+b)=j$.

With $P$, we can define some related measures:

\begin{itemize}
\item Entropy: $-\sum_{i,j} P[i,j] \log P[i,j]$

Entropy is the level of randomness in the data that is being processed. Entropy is highest when all the elements in P[i,j] are the same. In terms of images, this means that there is no preferred gray level.

\item Energy:  $\sum_{i,j} P[i,j]^2$

Energy simply defined as the sum of all of the matrix values combined

\item Contrast: $\sum_{i,j} (i-j)^2 P[i,j]$

Contrast is a measure of variation in an image. The formula places greater weighting on entries away from the diagonal (due to the $(i-j)^2$ term), thus a high contrast would mean more pixel pairs satisfying $\mathbf{d}$ will be constructed with widely different gray level intensities.

\item Homogeneity: $\sum_{i,j} \frac{P[i,j]}{ 1 + | i-j |}$

Homogeneity is an opposite measure to Contrast. It places greater weighting on entries on the diagonal (due to the $1 + |i - j|$ denominator). A highly homogenous image would mean more pixel pairs satisfying $\mathbf{d}$ are constructed with similar gray level intensities.

\end{itemize}

One major downside of this method is that many $\mathbf{d}$'s have to be tried before arriving at a good one. It is also not good for textures that are composed of primitives of a certain shape/structure.

\subsubsection{Gray Level Run Length Method}

The gray level run-length method utilizes a gray level run matrix $P_\theta$, where $P_\theta(i,j)$ is the number of times an image contains a run of length j for pixels with value i in a certain direction of $\theta$. Long runs can be expected for images with coarse texture.

With $P_\theta$, we can define some related measures. We define the following as a normalization factor for our measures:

$$
T_\theta = \sum_{i=1} ^{N_g} \sum_{j=1} ^{N_r} P_\theta(i,j)
$$

where $N_g$ is the number of gray levels and $N_r$ is the number of run lengths

\begin{itemize}
\item {Short run emphasis inverse moments}

$$  
\frac{1}{T_\theta} \cdot \sum_{i=1} ^{N_g} \sum_{j=1} ^{N_r} \frac{P_\theta (i,j)}{j^2}
$$

This measure emphasizes short runs of a gray level image by dividing by $j^2$. Short run emphasis is large when there are lots of short runs of the same intensity.

\item {Long run emphasis moments}
  
$$
\frac{1}{T_\theta} \cdot \sum_{i=1} ^{N_g} \sum_{j=1} ^{N_r} j^2 P_\theta (i,j)
$$

This measure emphasizes long runs of a gray level image by multiplying by $j^2$. Long run emphasis is large when there are lots of long runs of the same intensity.

\item {Gray level nonuniformity}
  
$$
\frac{1}{T_\theta} \cdot \sum_{i=1} ^{N_g} \left(\sum_{j=1} ^{N_r} P_\theta (i,j)\right)^2
$$

The sum inside of the brackets is the total number of runs for a certain gray level value. This value is large if the runs are not evenly distributed across different intensities.

\item {Run length nonuniformity}
  
$$
\frac{1}{T_\theta} \cdot \sum_{j=1} ^{N_r} \left(\sum_{i=1} ^{N_g} P_\theta (i,j)\right)^2
$$

The sum inside the brackets is the total number of runs with a certain run length at a certain gray level. This value is large if intensities are not evenly distributed across different run lengths.

\item {Inverse of weighted average run length}
  
$$
T_\theta\left(\sum_{i=1}^{N_g} \sum_{j=1} ^{N_r} j \cdot P_\theta (i,j)\right)^{-1}
$$

This measure puts more weight into longer run length.

\end{itemize}

\subsection{Structural Methods}

The basic assumption in structural methods is that textures are composed of primitives known as "texels".

\begin{itemize}
  \item {Works better than statistical methods if the primitives can be detected}
  \item {Geometric properties (size, elongation, orientation, etc.) of the primitives are important features}
  \item {Spatial relationship of the primitives based on the co-occurrence of these primitives can be further explored}
\end{itemize}

For example, if we use an edge-based method, then the primitives are edges. The properties are orientation, gradient, and density. The spatial relationship is edge separations.

\subsubsection{Tamura's Texture Measures}

Tamura's features are based on psychophysical studies of the characterizing elements that are perceived in textures by humans. Some of those features include"

\begin{itemize}
  \item {Coarseness}
  \item {Contrast}
  \item {Directionality}
\end{itemize}

\section{Classical Hough Transform}

The Classical Hough Transform is good for detecting lines and curves. It exploits the property that for analytical curves, dual spaces can be formed.

For detecting a line, the most naive parametrization of a line is $y = mx + c$. Suppose we have an $(x,y)$ space and a $(m,c)$ space. The line $y = mx + c$ in $(x,y)$ space corresponds to a point in the $(m,c)$ space. Similarly, a line in $(m,c)$ space corresponds to a point in $(x,y)$ space. There are at least two points needed in $(x,y)$ space to identify a line, and as such two corresponding lines in $(m,c)$ will intersect and form a point pinpointing the parameters $(m,c)$ needed to form a line in $(x,y)$ space.

The disadvantage of this parametrization is that a vertical line have $m = \infty$, and that would imply a huge $(m, c)$ space.

\subsection{Voting Algorithm}

The hough transform uses a process of voting for potentional matches of the geometric object needed, and by indentifying the most voted parameters the object in question can be detected. The following outline follows line detection with the parametrization $y = mx + c$

\begin{enumerate}
  \item Form a voting space with max and min values of $c$ and $m$
  \item For each edge point $(x,y)$ with gradient magnitude $S$ greater than a set threshold, increment all points in the voting space that satisfies $c = -xm + y$
  \item Identify maxima and peaks in the voting space to find lines in the original image
\end{enumerate}

\subsection{Polar parametrization of Lines}

As mentioned before, vertical lines cause $m = \infty$, which makes implementing a voting space difficult. As such, we can reparametrize a line to be following polar coordinates instead. A line will be described by the tangent line to a circle of radius $\rho$ at angle $\theta$. The relationship between $x, y, \rho, \theta$ is described as the following:

$$
\rho = x\cos\theta+y\sin\theta
$$

Depending on the range of $\rho$ and $\theta$, we can have $0 \leq \theta \leq 2\pi$ with $0 \leq \rho$. Alternatively, we can also have $0 \leq \theta \leq \pi$ with $\rho$ being positive or negative.

Because images are discrete entities, the margin of error grows the bigger as a point $(x,y)$ on a line goes further away from the origin: $\delta \rho \approx d \delta \theta$. One way to mitigate this issue is to perform this parametrization hierarchically by splitting the original image into quadrants and shifting the center point to minimize $\rho$.

\subsection{Gradient Information}

Using gradient information can be helpful because now a vote only needs to be casted in the direction of the gradient and not over all possible points.

\subsection{Discussion}

Hough transforms can recognize any curve of the form $f(\mathbf{x}, \mathbf{a}) = 0$, where $\mathbf{a}$ is a parameter vector. For example, for a circle defined as 

$$
(x-a)^2 + (y-b)^2 = r^2
$$

There are three parameters and as such the parameter space will be 3-dimensional.

The merit in using the hough transform is that it works for broken lines and curves, and that it is insensitive to noise. However, with increasing amount of parameters it becomes computationally expensive to execute.

\section{Generalized Hough Transform}
\subsection{R-Table Generation}

 \section{Representations of 2D Geometric Structures}
 \subsection{Boundary Representations}
 \subsubsection{Polyline}

  Polyline can be used to apporximate curves and contours accurately.
  
  The algorithm for curve approximation using polyline involves iteratively going through vertices, and adding new vertices if the maximum distance between the polyline and the shape is greater than $T_{max}$.

  For closed polylines, such as polygon, the area is $\frac{1}{2}\sum_{i=0}^{n-1}(x_{i+1}y_i + x_iy_{i+1})$.

 \subsubsection{Chain Code}

  Chain code is an economic coding method that has either 4 or 8 neighbours.
  The derivative of the chain code is equivalent to the difference of the chain code modulo 4 (or 8).
  A pro of this method is that it is rotation invariant.

  For the area of an enclosed space:
  For each element of the chain code in counterclockwise order, there are 4 cases
  
  \vspace{2mm}
  
  Case 0: area = area - y

  \vspace{2mm}

  Case 1: y = y + 1

  \vspace{2mm}

  Case 2: area = area + y

  \vspace{2mm}

  Case 3: y = y - 1

 \subsubsection{Curvature Scale Space (CSS)}

  Find the zero crossings of the image's contour (inflection points).
  The positions of these inflection points are relative to the length of the contour curve.
  The boundary function is convolved with a Gaussian kernel of increasing width.
  At each $\sigma$, the curvature function is computed and the zero crossing points are identified and plotted.
  As $\sigma$ increases, the contour will lose its inflection points and become a convex blob.
  
  \vspace{5mm}
  
  When $\sigma$ increases, no new inflection points are created, but their locations are shifted.
  More prominent structures in the contour can survive large values of $\sigma$.
  The localization problem of inflection points can be solved by tracking their locations in the scale-space image back to the original.
  It is a robust method, that is also fast and compact.
  CSS can be used for shape comparison, finding similarities and differences between images.

  \vspace{5mm}

  In a shape context descriptor, the image is separated into several bins, and you count the number of points that fall within each bin. This displays a distribution of points relative to each point.
  Solve for least cost assignments, $C_{ij}$ to get correspondences. And then use a template given by the correspondences.

 \subsection{Region Representations}
 \subsubsection{Spatial Occupancy Array}

  This method is simple and results in a sparse array.

 \subsubsection{Axis-based Representations}

  This method involves putting images on a grid and figuring out which x values correspond with a specific y value of the image (vice versa for x-axis).
  A con of this method is that it usually requires a combination of both X- and Y-axis representations.

 \subsubsection{Quad-trees}
 % split and merge

  A pro of this method is that it is a hierarchical tree representation. A con is that the representation depends on the chosen grid size. Also objects can't be easily shifted or scaled.
  
  \vspace{5mm}

  Algorithm: Split and Merge

  \vspace{5mm}

  This algorithm uses a quad tree for region growing. You start at an intermediate level, such as 64 x 64, then do a bottom up process (merge), and a top down process (split).
  During quad-tree construction, pixels in the pyramid are either black or white.
  You intersect two quad trees R1 and R2, and do the following steps:
  If R1 is black, then return R2. If R1 is white, return white. If R2 is black, return R1. If R2 is white, return white.
  Next, you determine the elements of the new tree, $R_{new}$.
  The NW element of $R_{new}$ is the intersect between NW R1 and NW R2.
  The NE element of $R_{new}$ is the intersect between NE R1 and NE R2.
  The SW element of $R_{new}$ is the intersect between SW R1 and SW R2.
  The SE element of $R_{new}$ is the intersect between SE R1 and SE R2.

 \subsubsection{Medial Axis Transform (MAT)}

 % continue from here from "distances"

  MAT is a thinning algorithm to get the skeleton of the region.
  For each point p, find the closest neighbors on the region boundary.


\subsection{Distances}
\subsection{Feature Space}
\subsection{K-means Clustering}
\subsection{Mean Shift Algorithm}
\subsection{Gaussian and Laplacian Pyramids}

\section{Representations of 3D Structures}
\subsection{Surface Representations}
\subsubsection{Polyhedral Surfaces}
\subsubsection{Generalized Cylinders}
\subsection{Volumetric Representations}
\subsubsection{Spatial Occupancy Array}
\subsubsection{Octree}
\subsubsection{Constructive Solid Geometry}
\subsubsection{Shape Histogram}
\subsection{Viewer-Centered Representation}
\subsection{Object-Centered Representation}

\end{multicols}

\end{document}
