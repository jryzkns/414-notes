\documentclass{article}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[landscape, margin=0.25in]{geometry} % lots more margin
\pagenumbering{gobble} % ignore page numbers

\usepackage{titling}
\setlength{\droptitle}{-0.75in}

\title{CMPT 414 review notes}
\author{Jack Zhou, Melanie Xu}
\date{}

\setlength{\parindent}{0cm}

\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref} % for nice looking urls
\usepackage{booktabs} % for making tables
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{multicol}
\usepackage{float}

\begin{document}

% \maketitle

\begin{multicols}{3}

\section{Image Processing}

In this section we outline methods that can be used to modify images. For our purposes we only concern ourselves with grayscale images. For color images, we can apply the same methods after transforming the color space into one that has separatable luminance and chrominance components (ex. YCbCr).

\subsection{Histogram Transformations}

An image histogram is constructed by the frequencies of pixel intensities. By modifying pixel intensities on a histogram-level we can alter and often improve the quality of images. The transformations are applied to the image histogram, but changes in histogram values reflect the mapping of pixel values in the image. We outline two methods below:

\subsubsection{Linear Stretching}

Sometimes the image histogram may only be occupying part of the intensity range (ex. mostly dark pixels or mostly light pixels). As a result, the image would have bad contrast.

One way to address this issue is to stretch the histogram so that it occupies the entire intensity range. To do this, we mark the top and bottom of the histogram (\texttt{a} and \texttt{b}, respectively) and for each pixel in the histogram, compute a proportion of where it stands between \texttt{a} and \texttt{b}. Based on these proportions, the histogram is then mapped to occupy the entire intensity range.

The algorithm is as follows. \texttt{p\_0} and \texttt{p\_m} are the minimum and maximum pixel intensities respectively.

\begin{verbatim}
function stretch(img, a, b, p_0, p_m)
  for pix in img:
    if     pix <= a: pix = p_0
    elseif pix >= b: pix = p_m
    else:  pix = (p_m-p_0)/(b-a)*(pix-a)+p_0
end
\end{verbatim}

One quick note is that we can set \texttt{a} and \texttt{b} to not be the max and min of the histogram, which then makes some pixels clip to the min or max.

\subsubsection{Histogram Equalization}

Histogram Equalization is another popular method that is often used for improving the constrast of the messages. The idea of Histogram Equalization is to transform the intensity histogram to be a uniform histogram. In practice, the resulting histogram will not be perfectly uniform. 

In essence: by spacing out pixel intensities, regions in the image where there are lots of detail but only represented with a few intensities become easier to process.

The algorithm is as follows: \texttt{hist} is the original histogram of the image, and \texttt{n\_factor} is the ratio between amount of pixels in the image and the levels of pixel intensities.

\begin{verbatim}
function hist_eq (img, hist, n_factor)
  for pix in img:
    pix = ceil(sum(hist[:p])/n_factor) - 1
end
\end{verbatim}

\subsection{Convolutions and filtering} 

A convolution is a linear and shift-invariant operator. Shift invariance implies that if $y(n)$ is the response to $x(n)$, then $y(n-k)$ is the response to $x(n-k)$. Linearity is defined in the usual manner. For our purposes, a 2D convolution is equivalent to filtering an image, where a filter is defined by a matrix that we call a mask.

For an image $f$ and a mask $t$, a 2D convolution is defined as the following:

\[
\begin{aligned}
f * t &= \iint_{-\infty}^{\infty} f(x-x', y-y') \cdot t(x',y') dx'dy'\\
      &= \sum_{x' = -M}^{M}\sum_{y' = -N}^{N} f(x-x', y-y') \cdot t(x', y')
\end{aligned}    
\]

For the discrete case, the mask has to be in dimensions of $(2M+1) \times (2N + 1)$.

\subsubsection{Cross-Correlations}

A cross-correlation between $f$ and $t$ is defined as 
$$
R_{ft}(x, y) = \sum_{x' = -M}^M \sum_{y' = -N}^N f(x+x', y+y') t(x',y')
$$

The above definition is not invariant to the magnitude of $f$. A normalized $R_{ft}$ is needed:

$$
R_{ft} = \frac{\sum_{x', y'}f(x+x', y+y') t(x',y')}{\sqrt{\sum_{x',y'}f(x+x', y+y')^2 \cdot \sum_{x',y'}t(x',y')^2}}
$$

\subsubsection{Mask Design}

Masks are usually matrices whose values are designed in a specific way to accomodate different uses. 

In computer vision, we usually use masks that are symmetric. This implies that we do not need to concern ourselves with demarcating convolutions and cross-correlations.

When the same mask is applied twice (for example, applied onto itself), we will see that the mask grows in size. For example:

$$
( \;1\; 1\; 1\; ) * ( \;1\; 1\; 1\; ) = ( \;1 \;2 \;3 \;2 \;1 \;)
$$

It is implied that mask values are normalized, so the previous example should really be

$$
\left( \;\frac{1}{3}\; \frac{1}{3}\; \frac{1}{3}\; \right) * \left( \;\frac{1}{3}\; \frac{1}{3}\; \frac{1}{3}\; \right) = \left( \;\frac{1}{9} \;\frac{2}{9} \;\frac{3}{9} \;\frac{2}{9} \;\frac{1}{9} \;\right)
$$

For a target 2D mask, it may be worthwhile to decompose into a kronecker product of two 1D masks, For example:

$$
\left(\begin{matrix} 1& 0 & -1 \end{matrix}\right) \otimes \left(\begin{matrix} 1\\ 2\\ 1 \end{matrix}\right) = \left(\begin{matrix} 1 & 0 & -1\\ 2 & 0 & -2\\ 1 & 0 & -1 \end{matrix}\right)
$$

A better visualization is the following, each entry in the matrix is the product of the row value and column value:
\begin{figure}[H]
  \centering
\begin{tabular}{c|ccc}
  & \textbf{1} & \textbf{0} & \textbf{-1}\\
  \hline
  \textbf{1} & 1 & 0 & -1\\
  \textbf{2} & 2 & 0 & -2\\
  \textbf{1} & 1 & 0 & -1
\end{tabular}
\end{figure}

The merit in decomposing the mask is to do with the time complexity of applying a filter. Each entry in the mask is used in a multiplication once for each pixel in the image (suppose the image is $N \times N$). Thus for a 2D $k \times k$ mask the time complexity will be $O(k^2N^2)$. However, for two 1D $k \times 1$ masks the time complexity will only be $O(kN^2)$.

\subsubsection{Frequency Analysis}

Images in the $(x,y)$ domain can be decomposed into sums of spatial frequencies in frequency domain via an integral transform mechanism (ex. DCT, ADST, etc). Spatial frequencies can be categorized into high frequencies or low frequencies. Applying a filter that attenuates (reduces) high frequencies while passing low frequencies is called a low-pass filter. The opposite is called a high-pass filter. The demarcation for a filter to distinguish between low and high frequency is called a cut-off frequency.

Suppose we have an integral transform $\mathcal{I}$, there are important properties to exploit. In the foremost, a convolution in image space is translated to a multiplication in frequency space:

$$
\mathcal{I}(f * t) = \mathcal{I}(f) \times \mathcal{I}(t)
$$

\subsection{Smoothing}

Smoothing is a low-pass filtering operation. It is also called de-noising. While smoothing will remove noise, it will also remove the sharpness of the image, resulting in a blurred effect. In a sense, de-noising the image is achieved by smearing the image.

\subsubsection{Unweighted Averaging}

One naive approach to blur an image is to use a $k \times k$ mask that with all the same entries (i.e. all 1's; keep in mind that normalization is implied). For example, such a $4 \times 4$ mask would look like this:

$$
\left[\begin{matrix} 1&1&1&1\\1&1&1&1\\1&1&1&1\\1&1&1&1\\\end{matrix}\right]
$$

\subsubsection{K-nearest Neighbor Averaging}

This method involves investigating a $m \times m$ neighborhood around each image pixel. $K$ neighbors that are nearest to the centre image pixel value are taken and their average is preserved. This method is edge-preserving, but non-linear.

\subsubsection{Median Filtering}

This method also involves investigating a $m \times m$ neighborhood, but instead the median of the neighborhood is taken. This method is also very effective against salt and pepper (S\&P) noise.

This method is also edge-preserving and non-linear, and additionally it does not preserve corners. One way to mitigate this issue is to choose a different set of values to observe in the neighborhood. For example, in a cross shape instead of the entire neighborhood.

\subsubsection{Gaussian Filtering}

Gaussian filtering is a popular method for denoising images, it is also used as a primitive in many other applications in computer vision. This is because the operation resembles optical blur, and a Gaussian mask has good mathematical properties.

The filter is parametrized with two parameters $(\sigma^2_x,\sigma^2_y)$, but we will treat the filter as if it has only one parameter $\sigma^2 = \sigma^2_x = \sigma^2_y$.

One important mathematical property of a Gaussian is that the integral transform of a Gaussian is still a Gaussian. For a Gaussian function $G(x)$ with variance $\sigma^2$, the integral transform response of the Gaussian $\mathcal{I}(G(x))$ has variance $\sigma_f^2$ where the relationship between $\sigma$ and $\sigma_f$ is described as $\sigma \cdot \sigma_f = (2\pi)^{-1}$. As such, the larger $\sigma$ is, the lower $\sigma_f$ is, which gives a lower cut-off frequency.

\subsection{Sharpening}

Sharpening is a high-pass filtering operation. It is the opposite of smoothing and as such it is sometimes called de-blurring.

\subsubsection{Laplacian-based Methods}

The Laplacian (2nd derivative) sharpening operator is defined as $f - \nabla \cdot \nabla f$. The Laplacian $\nabla \cdot \nabla f = \frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2}$ is derived as follows:

\[
\begin{aligned}
  \frac{\partial^2f}{\partial x^2} &= f(x+1,y) - f(x,y) - (f(x,y) - f(x-1,y))\\
  & = f(x+1,y) + f(x-1,y) - 2f(x,y) \\ 
  \frac{\partial^2f}{\partial y^2} &= f(x,y+1) - f(x,y) - (f(x,y) - f(x,y-1))\\
  & = f(x,y+1) + f(x,y-1) - 2f(x,y)
\end{aligned}
\]

As such, $\nabla^2 f(x,y) = f(x+1,y) + f(x-1,y) + f(x,y+1) + f(x,y-1) - 4f(x,y)$, which is similar to a blurred version of the image. The mechanism that a laplacian operator operates on is simply to subtract the noise from the image by subtracting a noisy version of the image: $\hat f(x,y) = 5f(x,y) - [f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)]$. This corresponds nicely to the following mask:

$$
\begin{bmatrix}
  0 & -1 & 0\\
  -1 & 5 & -1\\
  0 & -1 & 0
\end{bmatrix}
$$

A common variation on this operator is to give weight to the laplacian, so the operator is defined as $f - w \nabla \cdot \nabla f$. While this method is effective at sharpening the image, it also creates lots of artefacts and noise.

\subsubsection{Unsharp Masking}

In a similar spirit to the laplacian operator, unsharp masking (USM) operates by adding a scaled and inverted version(inversion is the same as subtraction) of the blurred image to the original. The blurring mechanism is to simulate the noise model using a blurring mechanism in the image instead of using a derivative based method. USM generally performs better than the Laplacian operator, but it will take more time to run as it is more than just one simple filter.

\section{Image Moments}

For an image $f(x,y)$, the $(p,q)$\textsuperscript{th} order moment is defined as the following:

\[
\begin{aligned}
  m(p,q) &= \iint x^p y^q f(x,y) dxdy\\
         &= \sum_x \sum_y x^p y^q f(x,y)
\end{aligned}  
\]

Suppose we have a segmented image represented by a binary image $B$, image moments can tell us different statistics about the image.

$m(0,0)$ for the image tells us the amount of pixels that are 1 in $B$, or the area of 1's:

$$
A = \sum_i \sum_j B[i,j]
$$

The first moment in either axis helps us identify the centroid of 1's:

$$
\bar x = \frac{1}{A}\sum_i \sum_j j \cdot B[i,j] \;\;\; \bar y = \frac{1}{A} \sum_i \sum_j i \cdot B[i,j]
$$

The axis of the least 2nd moment is the axis of elongation (orientation) for 2D regions.

\section{Template Matching}

Similarity between $M \times N$ image $f$ and a $k \times s$ template $t$ can be measured by their Euclidean distance, but the sum of squares itself will do:

\[
\begin{aligned}
  &d^2(x,y)\\
  =&\sum_{k,s}[f(x+k,y+s) - t(k,s)]^2 \\
  =&\sum_{k,s}[f(x+k,y+s)^2 -2f(x+k,y+s)t(k,s) + t(k,s)^2]
\end{aligned}
\]

If a perfect match is found at $(x',y')$, then $d^2(x',y') = 0$.

Our goal is to minimize $d^2(x,y)$. $f(x+k,y+s)^2$ is independent from t, and $t(k,s)^2$ is a constant. Therefore we only need to focus on maximizing $\sum_{k,s}f(x+k,y+s)t(k,s)$ to minimize $d^2(x,y)$, which is in effect looking for minimas in a cross-correlation.

\section{Edge Detection}

Edges are defined as intensity discontinuities in images.

\subsection{Criteria for Good Edge Detection}
\begin{itemize}
\itemsep0em
\item Always find real edges, not false ones
\item Good localization, where found edges are as close to the original as possible  
\item Unique Output
\end{itemize}

\subsection{Gradient-Based Methods}

In this category of methods, the image gradient is computed in one way or another to derive edge information.

\[
\begin{aligned}
  \frac{\partial f}{\partial x} = f(x+n, y) - f(x,y) \;& \;\frac{\partial f}{\partial y} = f(x, y+n) - f(x,y)\\
  S = \sqrt{\left(\frac{\partial f}{\partial x}\right)^2 + \left(\frac{\partial f}{\partial y}\right)^2} \;&\; \theta = \arctan\left(\frac{\partial f}{\partial x} \middle/ \frac{\partial f}{\partial y}\right)
\end{aligned}
\]

Usually, $n=1$ for computing the partial derivatives. In this context, $S$ is the gradient magnitude, and $\theta$ is the directional information of the gradient. All of these are computed over the entire image, so there's a map of all of these values corresponding to each pixel.

\subsubsection{Gradient Thresholding}

Simply put, if the magnitude $S$ is greater than a threshold $t$, then it is an edge pixel, and an edge map can be generated from it.

\subsubsection{Roberts Operator}

The roberts operator computes $S$ and $\theta$ differently. Instead of using $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$, it generates two filtered images with the following two masks:

$$
G_x = \left[\begin{matrix}1 & 0\\ 0 & -1\end{matrix}\right] \quad\quad G_y = \left[\begin{matrix}0 & 1\\ -1 & 0\end{matrix}\right]
$$

$S$ and $\theta$ are derived by the following:

$$
S = \sqrt{\left(G_x * f\right)^2 + \left(G_y * f\right)^2} \;\; \theta = \arctan\left(\frac{G_y * f}{G_x * f}\right) - \frac{3\pi}{4}
$$

The rest follows a simple thresholding procedure.

\subsubsection{Prewitt and Sobel Operators}

Prewitt uses 8 operators for each of the 8 ordinal-cardinal directions. The $0^{\circ}$ and $45^{\circ}$ operators are as follows:

$$
\begin{bmatrix}
  -1 & 0 & 1\\
  -1 & 0 & 1\\
  -1 & 0 & 1
\end{bmatrix}\quad
\begin{bmatrix}
  0 & 1 & 1\\
  -1 & 0 & 1\\
  -1 & -1 & 0
\end{bmatrix}
$$

This method includes providing directional information and uses local averages to reduce noise. However, the edge width can be greater than 1, which means there isn't a single output.

The Sobel operator is similar to Prewitt's, except it places a stronger emphasis on the weighting of the direction each mask is meant for.

$$
\begin{bmatrix}
  -1 & 0 & 1\\
  -2 & 0 & 2\\
  -1 & 0 & 1
\end{bmatrix}\quad
\begin{bmatrix}
  0 & 1 & 2\\
  -1 & 0 & 1\\
  -2 & -1 & 0
\end{bmatrix}
$$

\subsection{Laplacian/Zero Crossings}

This laplacian (second derivative) edge operator we describe here is different from the sharpening operator. Namely, the laplacian edge operator is 

$$
\begin{bmatrix}
  0 & -1 & 0\\
  -1 & 4 & -1\\
  0 & -1 & 0
\end{bmatrix}
$$

Whereas the laplacian sharpening operator has a midle entry of 5 instead of 4.

After applying the operator, we look for zero-crossings in the resulting image to determine where the edges are. 

Zero crossings are simply where there is a negative value and a neighboring positive value.

\subsection{Marr's Operator (LOG/DOG)}

Marr's Operator applies a gaussian blur $G$ to the image $f$ before calculating the laplacian to find zero crossings:

\[
\begin{aligned}
  \nabla^2 (G(x,y) &* f(x,y))\\
  (\nabla^2 G(x,y)) &* f(x,y)
\end{aligned}  
\]

Thus the operator is also called a laplacian of gaussian (LOG) operator. The gaussian's $\sigma$ can be tuned for different spatial frequencies.

Additionally, a laplacian of gaussian can be approximated by the difference between two gaussians, called a difference of gaussians (DOG).

\subsection{Canny Edge Operator}

\begin{enumerate}
\itemsep0em
\item \textbf{Smoothing}

Apply Gaussian Blur to the image $I' = G*I$.

\item \textbf{Gradient Operator}

Use $0^{\circ}$ and $90^{\circ}$ Sobel operators to generate $\frac{\partial I'}{\partial x}$ and $\frac{\partial I'}{\partial y}$. Using these, generate $S$ and $theta$.

\item \textbf{Non-maximum Suppression}

Non-maximum suppression thins out the edges that result from the gradient operator.

For each pixel $p$ in $S$, examine two neighbors along the direction of $\theta$. The pixel will be suppressed (set to 0) if $p$ is not the largest value among its neighbors, and the directions of its neighbors don't differ from $p$ by more than $45^{\circ}$.

\item \textbf{Double Thresholding}

Double thresholding determines the strength of each pixel, whether it's weak, strong, or irrelevant.

There are two thresholds, a high threshold $Th_h$ to tell apart strong and weak edge pixels. A low threshold $Th_s$ tells apart weak and irrelevant pixels. Strong and weak pixels are given particular values. The resulting matrix will only have three unique values.

\item \textbf{Hysteresis Tracking}

% This is used to determine if weak edge pixels are going to be transformed into strong edge pixels and included in the final edge map. Otherwise they are discarded.

If a weak edge pixel has at least one strong edge pixel surrounding it, then it is turned into a strong pixel. Otherwise it is discarded.

The final edge map is defined as the map of all the strong edge pixels.

\end{enumerate}

% \section{Regions and Segmentation}

% A region is a group of connected pixels with similar properties in an image.

% \subsection{Image Segmentation}

% Image segmentation is a process to divide an image into regions. It can be done in a manner that is region-based, edge-based, or in a hybrid approach.

% Some of the methods for Image Segmentation are as follows
% \begin{itemize}
%   \item {Thresholding}
%   \item {Blob colouring (for binary images)}
%   \item {Split and Merge}
%   \item {K-Means}
%   \item {Mean Shift}
% \end{itemize}

% \subsection{Gestalt Psychology and Human Perception}

% Grouping is the key to visual perception. The Law of Pragnanz in Gestalt psychology states that people perceive ambiguous or complex images as the simplest form possible.

% Here are some of the heuristics that can be used to group items together.
% \begin{multicols}{3}
% \begin{itemize}
%   \item {Proximity}
%   \item {Similarity}
%   \item {Common Fate}
%   \item {Common Region}
%   \item {Parallelism}
%   \item {Symmetry}
%   \item {Continuity}
%   \item {Closure}
%   \item {Familiar Shapes}
% \end{itemize}
% \end{multicols}

\section{Texture Analysis}

Textures are defined as repeated patterns of local intensity variations, and it depends on the resolution. There are three issues related to texture analysis:

\begin{itemize}
  \itemsep0em
  \item {texture classification}
  \item {texture segmentation}
  \item {shape from texture}
\end{itemize}

\subsection{Spatial Gray Level Dependence}
bruh % do not remove this bruh or the page will collapse onto itself!
Spatial gray level dependence is a statistical method utilizing a gray level co-occurence matrix $P$. It is a $M \times M$ where $M$ is the amount of gray levels. $P$ serves as a tally of pixel-value pairs that follow a predefined pixel structure $\mathbf{d}$. Due to the particularity of $\mathbf{d}$, $P$ is not symmetric. $P$ is also normalized, and as such treated as a pmf.

$\mathbf{d}$ is defined as a pair of integral values $(a,b)$ which is to be interpreted as "starting with pixel intensity $i$, move $a$ units to the right then $b$ units down to get to pixel intesity $j$". In other words, a valid pixel pair satisfying $\mathbf{d}$ is $f(x,y)=i$ and $f(x+a,y+b)=j$.

\begin{itemize}
\itemsep0em
\item Entropy: $-\sum_{i,j} P[i,j] \log P[i,j]$

Level of randomness in the data that is being processed. Highest when all the elements are the same. In terms of texture, this means there is no preferred gray level.

\item Energy:  $\sum_{i,j} P[i,j]^2$

Sum of all of the matrix values combined.

\item Contrast: $\sum_{i,j} (i-j)^2 P[i,j]$

Greater weighting on entries away from the diagonal due to $(i-j)^2$ factor. Texture has more pairs satisfying $\mathbf{d}$ with widely different gray levels.

\item Homogeneity: $\sum_{i,j} \frac{P[i,j]}{ 1 + | i-j |}$

Greater weighting on entries on the diagonal due to the $1 + |i - j|$ denominator. Texture has more pairs satisfying $\mathbf{d}$ with similar gray levels.

\end{itemize}

One major downside of this method is that many $\mathbf{d}$'s have to be tried before arriving at a good one. It is also not good for textures that are composed of primitives of a certain shape/structure.

\subsection{Gray Level Run Length}

Gray level run-length is a statistical method that utilizes a gray level run matrix $P_\theta$, where $P_\theta(i,j)$ is the number of times an image contains a run of length j for pixels with value i in a certain direction of $\theta$. Long runs can be expected for images with coarse texture.

With $P_\theta$, we can define some related measures. We define the following as a normalization factor for our measures:

$$
T_\theta = \sum_{i=1} ^{N_g} \sum_{j=1} ^{N_r} P_\theta(i,j)
$$

where $N_g$ is the number of gray levels and $N_r$ is the number of run lengths

\begin{itemize}
\itemsep0em
\item {Short run emphasis inverse moments}

$$  
\frac{1}{T_\theta} \cdot \sum_{i=1} ^{N_g} \sum_{j=1} ^{N_r} \frac{1}{j^2}P_\theta (i,j)
$$

This measure emphasizes short runs of a gray level image by dividing by $j^2$.

\item {Long run emphasis moments}
  
$$
\frac{1}{T_\theta} \cdot \sum_{i=1} ^{N_g} \sum_{j=1} ^{N_r} j^2 P_\theta (i,j)
$$

This measure emphasizes long runs of a gray level image by multiplying by $j^2$.

\item {Gray level nonuniformity}
  
$$
\frac{1}{T_\theta} \cdot \sum_{i=1} ^{N_g} \left(\sum_{j=1} ^{N_r} P_\theta (i,j)\right)^2
$$

The sum inside of the brackets is the total number of runs for a certain gray level value. This value is large if the runs are not evenly distributed across different intensities.

\item Run length nonuniformity
  
$$
\frac{1}{T_\theta} \cdot \sum_{j=1} ^{N_r} \left(\sum_{i=1} ^{N_g} P_\theta (i,j)\right)^2
$$

The sum inside the brackets is the total number of runs with a certain run length at a certain gray level. This value is large if intensities are not evenly distributed across different run lengths.

\item Inverse of weighted average run length
  
$$
T_\theta\left(\sum_{i=1}^{N_g} \sum_{j=1} ^{N_r} j \cdot P_\theta (i,j)\right)^{-1}
$$

This measure puts more weight into longer run length.

\end{itemize}

\subsection{Structural Methods}

The basic assumption in structural methods is that textures are composed of primitives known as "texels".

\begin{itemize}
\itemsep0em
\item {Works better than statistical methods if the primitives can be detected}
\item {Geometric properties (size, elongation, orientation, etc.) of the primitives are important features}
\item {Spatial relationship of the primitives based on the co-occurrence of these primitives can be further explored}
\end{itemize}

For example, if we use an edge-based method, then the primitives are edges. The properties are orientation, gradient, and density. The spatial relationship is edge separations.

\subsubsection{Tamura's Texture Measures}

Tamura's features are based on psychophysical studies of the characterizing elements that are perceived in textures by humans. Some of those features include:

\begin{itemize}
\itemsep0em
\begin{multicols}{3}
\item {Coarseness}
\item {Contrast}
\item {Directionality}
\end{multicols}
\end{itemize}

\section{Classical Hough Transform}

The Classical Hough Transform is good for detecting lines and curves. It exploits the dual spaces for analytical curves.

For detecting a line, the most naive parametrization of a line is $y = mx + c$. The line $y = mx + c$ in $(x,y)$ space corresponds to a point in the $(m,c)$ space. Similarly, a line in $(m,c)$ space corresponds to a point in $(x,y)$ space. There are at least two points needed in $(x,y)$ space to identify a line, and as such two corresponding lines in $(m,c)$ space will intersect and form a point pinpointing the parameters $(m,c)$ needed to form the line in $(x,y)$ space.

\subsection{Voting Algorithm}

The hough transform uses a process of voting for potentional matches of the geometric object needed, and by indentifying the most voted parameters the object in question can be detected. The following outline follows line detection with the parametrization $y = mx + c$

\begin{enumerate}
\itemsep0em
\item Form a voting space with max and min values of $c$ and $m$
\item For each edge point $(x,y)$ with gradient magnitude $S$ greater than a set threshold, increment all points in the voting space that satisfies $c = -xm + y$
\item Identify maxima and peaks in the voting space to find lines in the original image
\end{enumerate}

\subsection{Polar parametrization of Lines}

As mentioned before, vertical lines cause $m = \infty$, which makes implementing a voting space difficult. As such, we can reparametrize a line to be following polar coordinates instead. A line will be described by the tangent line to a circle of radius $\rho$ at angle $\theta$. The relationship between $x, y, \rho, \theta$ is described as $\rho = x\cos\theta+y\sin\theta$

Depending on the range of $\rho$ and $\theta$, we can have $0 \leq \theta \leq 2\pi$ with $0 \leq \rho$. Alternatively, we can also have $0 \leq \theta \leq \pi$ with $\rho$ being positive or negative.

Because images are discrete entities, the margin of error grows the bigger as a point $(x,y)$ on a line goes further away from the origin: $\Delta \rho \approx d \Delta \theta$. One way to mitigate this issue is to perform this parametrization hierarchically by splitting the original image into quadrants and shifting the center point to minimize $\rho$.

\subsection{Gradient Information}

Using gradient information can be helpful because now a vote only needs to be casted in the direction of the gradient and not over all possible points.

\subsection{Discussion}

Hough transforms can recognize any curve of the form $f(\mathbf{x}, \mathbf{a}) = 0$, where $\mathbf{a}$ is a parameter vector. For example, for a circle defined as $(x-a)^2 + (y-b)^2 = r^2$ has three parameters and as such the parameter space will be 3-dimensional.

The merit in using the hough transform is that it works for broken lines and curves, and that it is insensitive to noise. However, with increasing amount of parameters it becomes computationally expensive to execute.

\section{Generalized Hough Transform}

While classical Hough Transforms are ideal for analytical curves, it is unable to work with arbitrary curves. Generalized Hough Transform (GHT) works by detecting objects based on the shape of its boundary. It is a generalized form of template matching. It contains two stages: Modeling and Matching.

\subsection{Modeling: R-Table Generation}

A template that is given to GHT is transformed into an R-table. An R-table is a lookup table of differences from edge points to a reference point. An R-table is built in the following way:

\begin{enumerate}
\itemsep0em
\item Determine template centroid $(x_c, y_c)$ as a reference point.
\item Initialize R-table as $Q$ bins, where $Q$ is the amount of intervals to divide $0^{\circ}$ to $360^{\circ}$ into.
\item For each edge point $(x, y)$ in the template, derive its gradient angle and compute $\phi$, which is the index of the bin that the entry belongs to.
\item place $(\Delta x, \Delta y) = (x_c-x,y_c-y)$ in bin $\phi$,
\end{enumerate}

In the original formulation, the entries are $(r_i, \theta_i)$ values that denote distances and angles from the centroid. Storing simple differences allows for simpler calculations for reconstruction of centroid locations when there is a scale $s$ and rotation $\theta$:

$$
\left[\begin{matrix}x_c \\ y_c \end{matrix}\right] = s \left[\begin{matrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{matrix}\right] \left[\begin{matrix} \Delta x \\ \Delta y\end{matrix}\right]
$$

\subsection{Matching: Voting Algorithm}

\begin{enumerate}
\itemsep0em
\item Generate edge map and gradient angles of the source image
\item Initialize the Hough Space with the image dimensions
\item For each edge pixel in the edge map:
\begin{itemize}
\itemsep0em
        \item compute $\phi$ to determine the bin to look up in the model (with compensation for rotation)
        \item For each entry in bin $\phi$:
        \begin{itemize}
        \itemsep0em
                \item Compute a candidate location (with compensation for rotation and scaling) for the model
                \item Cast a vote in the candidate location if it does not clip out of the image
        \end{itemize}
\end{itemize}
\end{enumerate}

\subsection{Discussion}

GHT is only able to detect with one orientation and scale at a time, which is kind of inefficient. Peak detection with the votes is also non-trivial. It can be used in multiple stages to construct more complex contours.

\section{Representations of 2D Geometric Structures}
\subsection{Polyline}

A polyline representation can be used to apporximate curves and contours accurately. It consists of an ordered array of $(x_i, y_i)$ vertices that approximate the true contours, where the vertices form a closed loop in clockwise fashion.

The algorithm for curve approximation using polyline involves iteratively going through vertices, and adding new vertices if the maximum distance between the polyline and the shape is greater than $T_{max}$.

Applying rotation on for a polyline representation requires applying a rotation matrix on every point of the representation.

For a closed polyline representation, its area is defined as $\frac{1}{2}\sum_{i}(x_{i+1}y_i + x_iy_{i+1})$

\subsection{Chain Code}

Chain code is an economic coding method where contours are approximated with a chain of lines that either go in the 4 or 8 directions (called neighbors) in a counterclockwise fashion. Each direction is enumerated from 0 to 3 or 7, and a chain code is represented as a list of these directions.

The derivative of the chain code is equivalent to the difference of the chain code at a position with its previous position modulo 4 or 8.

Applying rotation for chain coding is easily achieved by uniformly adding a rotation value to each value in the chain-code.

For the area of an enclosed by a 4-neighbor chain code, it can be computed in the following manner:

\begin{itemize}
  \itemsep0em
  \begin{multicols}{2}
  \item Case 0: area = area - y
  \item Case 1: y = y + 1
  \item Case 2: area = area + y
  \item Case 3: y = y - 1
  \end{multicols}
\end{itemize}

\subsection{Curvature Scale Space (CSS)}

A curvature scale space (CSS) is a representation that identifies objects by inflection points (ie. zero crossings) on their contour. The location of the inflection points are expressed as a function of relative arclength. 

The CSS of a contour is computed multiple times until no more inflection points are found. Each time after a gaussian kernel with increasing $\sigma$ is applied.

With more filtering, weaker details are removed thus less inflection points left. The inflection points that last the longest often point toward the most prominent features.

After computing CSS with all of the increasing $\sigma$, we obtain a graph that has many curves that grow upwards. The change in position in each curve as it grows upwards reflects the change of location of its inflection point as greater $\sigma$ is applied.

This representation is scale invariant, rotation invariant, and robust to noise.

\subsection{Shape Context Descriptor}

A shape context descriptor is a compact representation where the template image (containing its boundaries) is separated into partitions (bins). The number of points that fall within each bin forms a distribution.

To match an object with Shape Context Descriptors, solve for least cost assignment to get correspondences, and then use a template given by the correspondences.

\subsection{Spatial Occupancy Array}

This method is simple and results in a sparse array.

\subsection{Axis-based Representations}

This method involves putting images on a grid and figuring out which x values correspond with a specific y value of the image (vice versa for x-axis). 

The representation is a list of lists. Each list starts with the value at the primary axis, and then each pair of values that comes after is the starting and end value of the other axis (ex. $(y, x_{in}, x_{out}, x_{in}, x_{out})$).

This method is inefficient for specific cases and usually requires a combination of both X- and Y-axis representations.

\subsection{Quad-trees}

Using quad trees is a space efficient method where more detail is added where needed. It is a hierarchical but it depends on the chosen grid size. Quad tree representations also can't be easily shifted or scaled.

A quad tree is a complete 4-ary tree where node is either a grey, white, or black pixel. Black pixels denote the region is contained, white pixels denote that it is not a region, and only grey pixels have children. 

To form a Quad tree representation of a region, we can start at an intermediate level, and apply a split and merge technique. 

For splitting, the node is colored grey, then it forms 4 children and colors each accordingly.

For merging, the intersection of two quad trees is performed. Two nodes $R_1, R_2$ are taken as input. If either node is black or white, then that color is the resulting color (black has higher precedence than white). Else both nodes are grey pixels, then the intersection between all of their 4 children are computed in the same manner.

\subsection{Medial Axis Transform (MAT)}

With a given boundary, a medial axis transform can effectively compute the skeleton of the region.

For each point $p$, if there are more than one equidistant neighbors on the region boundary, it is a medial axis and as such part of the region's skeleton.

At each point $i$ in the region, find the largest disk centered at $i$ and only contained by the region. A point $p$ is maximal if its disk is not completely contained without another point $i$'s disk. The union of all the disks represents the object.

While generating skeletons of regions is very helpful, this method is noise sensitive.

\subsection{Distance Measures}

\begin{itemize}
\itemsep0em
\item $L_p$ distance
  
In $L_p$ space, the distance between between two vectors $\mathbf{a}$ and $\mathbf{b}$ is defined as

$$
||\mathbf{x}||_p = \left(\sum_{i} | x_i |^p \right)^{\frac{1}{p}}
$$

where $\mathbf{x} = \mathbf{a} - \mathbf{b}$. If we are talking about a single vector, then the distance is considered a $p$-norm.

\item City Block distance (Manhattan)

$L_p$ distance measure with $p=1$. Thus it is also called a $L1$ distance.

$$
d_4(\vec p_1, \vec p_2) = | x_1 - x_2 | + | y_1 - y_2 |
$$

\item Euclidean distance

$L_p$ distance measure with $p=2$. Thus it is also called a $L2$ distance.

$$
d_e(\vec p_1, \vec p_2) = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
$$

\item Chebyshev (Chess Board)

The Chebyshev distance between two vectors is defined as the following:

$$
d_8(\vec p_1, \vec p_2) = max\{| x_1 - x_2 |, | y_1 - y_2 |\}
$$

It is also called a chess board distance because it mimicks how a king piece moves on a chess board.

\end{itemize}

\subsection{Error Measures}

These error measures are commonly used and can also be used as loss functions. The following measures utilize a ground truth vector $\mathbf{y}$ and a prediction vector $\mathbf{\hat y}$

\begin{itemize}
\itemsep0em
\item Mean Square Error (MSE)

$$
MSE(\mathbf{\hat y}, \mathbf{y}) = \frac{1}{n}\sum_{t=1}^n (\hat{y_t} - y_t)^2
$$

\item Mean Absolute Error (MAE)

$$
MAE(\mathbf{\hat y}, \mathbf{y}) = \frac{1}{n}\sum_{t=1}^n \mid \hat{y_t} - y_t \mid
$$

\item Root Mean Square Error (RMSE)

$$
RMSE(\mathbf{\hat y}, \mathbf{y}) = \sqrt{\frac{1}{n}\sum_{t=1}^n (\hat{y_t} - y_t)^2}
$$

\end{itemize}

\subsection{Feature Space}

Often, objects can be represented with a series of tokens. Every token could represent some sort of feature such as position, color, texture, motion vector, size, or orientation.

A feature space is the choice of features and how they are quantified, where each token is represented by a point.

Token similarity is defined by a feature vector, which is the distance between points.

\subsection{K-means Clustering}

K-mean clustering is a method of classification of points in a feature space. effectively finding clusters of points. Given an input $K$ for the amount of clusters that should be found, the algorithm is as follows:

\begin{enumerate}
\itemsep0em
\item Partition the $N$ points in the feature space into $K$ intial sets. Calculate the initial set centroids $m_1, ..., m_k$.

\item While $m_1, ..., m_k$ has not stabilized, do:
\begin{enumerate}
\itemsep0em
\item {Assign each of the $N$ points to their nearest $m_i$, making sure that no cluster is empty}
\item {Recompute mean $m_i$ of each cluster}
\end{enumerate}
\end{enumerate}

Effectively, for each cluster $C_i$, the k-means algorithm carries out gradient descent to minimize

$$
\sum_{i}\sum_{j \in C_i} ||x_j - \mu_j||^2
$$

A sample use case with K-means is to identify the most important colors in an image and apply color segmentation.

\subsection{Mean Shift Algorithm}

One fatal issue with K-means cluster is that the number of clusters needs to be known beforehand. Mean shift locates maxima of a density function, given discrete data sampled from that function locally at the current estimates.

For each point, their original position is taken as the initial estimate of the mean $x$. A kernel function $K(x-x_i)$ is applied at $x$ against all $x_i$ points in a neighborhood. A new weighted mean is computed as the following

$$
m(x) = \frac{\sum _{x_i \in N(x)}[K(x_i-x) \cdot x_i]}{\sum _{x_i \in N(x)}K(x_i-x)}
$$

The estimate $x$ is now set as $m(x)$, and this process continues until convergence. The difference $m(x) - x$ is known as the mean shift vector, and it is proven to always point toward the maximum increase in density.

\subsubsection{Applications}

For Image Segmentation, the feature space is the Space-Range domain: space is the coordinate of each pixel, range is the grayscale or color vector of the pixel.

The kernel function would be the product $K_s(x_i^s - x^s) \cdot K_s(x_i^r - x^r)$, one for space and one for range.

After Mean Shift filtering, modes can be merged if they are closer than $h_s$ in the Space domain and $h_r$ in the Range domain. We can also merge regions that are too small.

\subsection{Gaussian Pyramid}

A Gaussian pyramid contains successively lower resolutions of an image.The process of generating the next image in the pyramid from the image before it is called a REDUCE operation. Both resolution and density are decreased.

$$
G_l(i,j) = \sum_{m = -2}^2 \sum_{n = -2}^2 w(m,n) G_{l-1} (2i+m, 2j+n)
$$

The function $w(m, n)$ is called a generating kernel (which is usually a matrix), and it has to satisfy the followng constraints:

\begin{enumerate}
\itemsep0em
\item {Separable: $w(m,n) = \hat{w}(m)\hat{w}(n)$}
\item {Normalized: $\sum \hat{w}(m) = 1$}
\item {Symmetric: $\hat{w}(m) = \hat{w}(-m)$}
\item {Equal Contribution: $a+2c = 2b$}
\end{enumerate}

After combining the second and fourth constraints, we have the following:

\[
\begin{aligned}
  a &= free\\
  b &= 1/4\\
  c &= 1/4 - a/2
\end{aligned}
\]

The inverse of the REDUCE operation is called the EXPAND operation. This operation increases the size of an image from $M+1$ to $2M+1$. Let $G_{l,k}$ be the image obtained by applying EXPAND to $G_l$ $k$ times, then $G_{l,0} = G_{l}$ and

$$
G_{l,k}(i,j) = 4\sum_{m = -2}^2 \sum_{n = -2}^2 w(m,n) G_{l,k-1}(\frac{i+m}{2}, \frac{j+n}{2})
$$

In this expression, only terms for when $(i+m)/2$ and $(j+n)/2$ are integers contribute to the sum $G_{l,k}(i,j)$.

Note that $G_{l,1}$ is the same size as $G_{l-1}$, and $G_{l,l}$ is the same size as the original image.

When EXPAND is applied to an image $l$ times, $2^l-1$ points are made between each pair of neighbouring samples.

\subsection{Laplacian Pyramids}

With a series of Gaussian Pyramid images, the corresponding laplacian pyramid images can be easily derived with the following algorithm:

For values $l$ from 0 to $N$, the laplacian pyramid image $L_l$ is defined as $G_l - G_{l+1,1}$, with $L_N = G_N$. In essence, each laplacian pyramid image is a difference of Gaussian pyramid images.

An important feature of the Laplacian pyramid is that the original image can be found by summing up pyramid levels. That is:$G_0 = \sum_{l=0}^{N}L_{ll}$

Due to the fact that laplacian images have this property, and each EXPAND operation effectively subtracts an average of neighboring values, the image data is compressed with the pixel to pixel correlation removed and values shifted toward zero. This effectively reduces the information intropy of each image and thus reduces transmission bandwidths.

\section{Representations of 3D Structures}
\subsection{Winged-Edge}

The winged-edge is polyhedral surface representation. It uses nodes which are of one of the following types:

\begin{itemize}
\itemsep0em
\item face: face characteristics (surface, normal, reflectance, color, etc.)
\item edge: topological information for faces, edges, and vertices, as well as additional information
\item vertex: 3D vertex location
\end{itemize}

From those nodes, a graph connecting them is created. One property to note with this representation is that it has to satisfy Euler's theorem $|V|-|E|+|F|=2$.

\subsection{Some polyhedral representations}

Other polyhedral surface representations include B-splines, point clouds, and 3D (polygonal) meshes.

\subsection{Shape Distribution}

A shape distribution of a 3D surface gives a 1D profile of itself by randomly sampling pairs of points on the surface and recording occurences of the distances in a histogram. The histogram is normalized into a distribution afterwards.

\subsection{Spin Image}

A spin image of a 3D surface gives a 2D profile of itself by rotating a plane around the surface's principal axis. At each angle, the set of points that touch the plane are recorded into a 2D histogram. The histogram is normalized into a distribution afterwards.

\subsection{Generalized Cylinders/Cones}

A Generalized Cylinder is a surface representation which defines a spine which circles grow from. It has a planar cross section, a space curve spine, and a sweep rule (constant, linear contraction, etc.).


\subsection{Spatial Occupancy Array}

This volumetric representation uses voxels, which are the simplest representation, but they are very wasteful. They can use sparse arrays, and one of the dimensions can be run-length coded.

\subsection{Octree}

Octrees are the 3D version of quadtrees which volumetrically represent objects. Many of its properties inherit from quadtrees. Each node has exactly 8 children (complete 8-ary tree), and each node is said to be void (white), full (black), or mixed (grey). Quad tree operations such as union, intersection, and complement of can be solved in similar method for octrees.
When given a $2^n \times 2^n \times 2^n$ array of voxels, the dimension of the array is said to be $2^n$. A node at level $i$ for an octree corresponds to an element which contains $2^i \times 2^i \times 2^i$ voxels.

\subsection{Constructive Solid Geometry}

Constructive Solid Geometry (CSG) schemes are volumetric representations that represent volumes as compositions of other primitive solids via set operations. Primitives such as arbitrarily scaled rectangular prisms, cylinders, cones, and spheres are used. A CSG representation is an expression that follows the following form:

\[
\begin{aligned}  
  <\mathtt{CSG}\; rep> ::= & <\mathtt{primitive}> | \\
  &\mathtt{MOVE} <\mathtt{CSG}\; rep> \mathtt{BY} <\mathtt{params}> | \\
  & <\mathtt{CSG}\; rep> <\mathtt{op}> <\mathtt{CSG}\; rep>
\end{aligned}
\]

The set operations are best taken to be their regularized version. In short, this means that set operations on solids should leave them with "tight skins", so no dangling surfaces or edges. Regularized operators are defined by

$$
X <OP> * Y =  r(X <OP> Y)
$$

If primitives are unbounded, then it is difficult to check for the boundedness of an object. However, if they are bounded, then any CSG representation is a valid volume representation.

\subsection{Shape Histogram}

The shape histogram is a volumetric representation that provides a 3D profile of a 3D object. It is constructed in the following manner:

\begin{enumerate}
\itemsep0em
\item A volumetric representation is created by filling in the surface with voxels
\item The space is transformed to a spherical coordinate $(r, \phi, \theta )$ system around the center of mass of the model
\item A 3D spherical histogram is constructed for accumulating voxels in the volume representation
\item The histogram is normalized
\end{enumerate}

\subsection{Viewer-Centered Representation}

Viewer-Centered Representation models 3D objects through a set of views ($2 \frac{1}{2}$ sketch - depth map, etc.). It is a bottom-up approach, but each 3D object must be represented by many views.

\subsection{Object-Centered Representation}

Object-Centered Representation involves expressing object geometry with respect to a reference frame, such as GCs, Octrees, Shape Histograms, etc. It is a top-down approach, and it is often better for invariance (translation, rotation, and scale).

\end{multicols}

\end{document}
