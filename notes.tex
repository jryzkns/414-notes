\documentclass{article}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.75in]{geometry} % lots more margin
\pagenumbering{gobble} % ignore page numbers

\usepackage{titling}
\setlength{\droptitle}{-0.75in}

\title{CMPT 414 review notes}
\author{}
\date{}

\setlength{\parindent}{0cm}

\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref} % for nice looking urls
\usepackage{booktabs} % for making tables
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{multicol}

\begin{document}

\maketitle

\begin{multicols}{2}

\section{Image Processing}

In this section we outline methods that can be used to modify images. For our purposes we only concern ourselves with grayscale images. For color images, we can apply the same methods after transforming the color space into one that has separatable luminance and chrominance components (ex. YCbCr).

\subsection{Histogram Transformations}

An image histogram is constructed by the frequencies of pixel intensities. By modifying pixel intensities on a histogram-level we can alter and often improve the quality of images. We outline two methods below:

\subsubsection{Linear Stretching}

Sometimes the image histogram may only be occupying part of the intensity range (ex. mostly dark pixels or mostly light pixels). As a result, the image would have bad contrast.

One way to address this issue is to stretch the histogram so that it occupies the entire intensity range. To do this, we mark the top and bottom of the histogram (\texttt{a} and \texttt{b}, respectively) and for each pixel in the histogram, compute a proportion of where it stands between \texttt{a} and \texttt{b}. Based on these proportios, the histogram is then re-mapped to occupy the entire intensity range.

The algorithm is as follows. \texttt{p\_0} and \texttt{p\_m} are the minimum and maximum pixel intensities respectively.

\begin{verbatim}
function linear_stretch (img, a, b, p_0, p_m)
  for pix in img:
    if     pix <= a: pix = p_0
    elseif pix >= b: pix = p_m
    else:  pix = (p_m-p_0)/(b-a)*(pix-a)+p_0
end
\end{verbatim}

One quick note is that we can set \texttt{a} and \texttt{b} to not be the max and min of the histogram, which then makes some pixels clip to the min or max.

\subsubsection{Histogram Equalization}

Histogram Equalization is another popular method that is often used for improving the constrast of the messages. The idea of Histogram Equalization is to transform the intensity histogram to be a uniform histgram. In practice, the resulting histogram will not be perfectly uniform. As a matter of fact, for dense peaks in the original histogram, the resulting equalized histogram will have discontinuities in that area.

By spacing out pixel intensities, regions in the image where there are lots of detail but only represented with a few intensities become easier to process.

The algorithm is as follows. \texttt{hist} is the original histogram of the image, and \texttt{n\_factor} is the ratio between amount of pixels in the image and the levels of pixel intensities. 

\begin{verbatim}
function hist_eq (img, hist, n_factor)
  for pix in img:
    pix = ceil(sum(hist[:p])/n_factor) - 1
end
\end{verbatim}

\subsection{Convolution and Filtering} 
% overview of frequency analysis
% low pass vs high pass filtering
% mask design
% Applying 2D Mask vs 1D Mask
% linear and shift invariance

Convolution is an integral that represents the overlap of one function t as it is shifted over another function f.

\[q(x,y) = \int \int_{-\infty}^{\infty} f(x-x', y-y') \cdot t(x',y') dx'dy'\]
\[f * t\]

In terms of image processing, convolution is adding each element in a matrix to its local neighbours, weighted by the convolution mask. The convolution mask is a small matrix that is applied to an image matrix to perform blurring, sharpening, etc.

Filtering helps remove either unwanted high frequencies or low frequencies, through the use of low-pass filters and high-pass filters.

The discrete case of convolution is as follows:

\[g(x) = \Sigma_{x'=-M}^{M} f(x-x') \cdot t(x')\]

2D masks can be composed from two 1D masks.

$\begin{pmatrix}
  \frac{1}{4} & \frac{1}{2} & \frac{1}{4}
\end{pmatrix}$
$\bigotimes$
$\begin{pmatrix}
  \frac{1}{4}\\
  \frac{1}{2}\\
  \frac{1}{4}
\end{pmatrix}$
=
$\begin{pmatrix}
  \frac{1}{16} & \frac{1}{8} & \frac{1}{16}\\
  \frac{1}{8} & \frac{1}{4} & \frac{1}{8}\\
  \frac{1}{16} & \frac{1}{8} & \frac{1}{16}
\end{pmatrix}$

According to the Associative Law for convolution, $(\vec I \bigotimes \vec m_1) \bigotimes \vec m_{1}^{T} = \vec I \bigotimes (\vec m_1 \bigotimes \vec m_{1}^{T})$

The runtime analysis for the left hand side is $O(\mid \vec m_1 \mid)$, and the runtime analysis for the right hand side is $O(\mid \vec m_1 \mid^2)$. $\mid \vec m_1 \mid$ is the size of $\vec m_1$.

A shift invariant system is the discrete equivalent of a time invariant system. If y(n) is the response of the system to x(n), then y(n-k) is the response of the system x(n-k).

Given two valid outputs, $x_1(t)$ and $x_2(t)$, as well as their outputs, $y_1(t) = H{x_1(t)}$ and $y_2(t) = H{x_2(t)}$, then a linear system must satisfy $\alpha y_1(t) + \beta y_2(t) = H{\alpha x_1(t) + \beta x_2(t)}$.

\subsection{Smoothing}

Smoothing an image is removing the noise from the image. While smoothing will remove noise, it will also remove the sharpness of the image, resulting in a blurred effect.

\subsubsection{Unweighted Averaging}

$t(x',y') =$
$\begin{pmatrix}
  \frac{1}{9} & \frac{1}{9} & \frac{1}{9}\\
  \frac{1}{9} & \frac{1}{9} & \frac{1}{9}\\
  \frac{1}{9} & \frac{1}{9} & \frac{1}{9}
\end{pmatrix}$

For example,
$f = $
$\begin{pmatrix}
  0 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 0\\
  0 & 9 & 9 & 9 & 0\\
  0 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 0\\
\end{pmatrix}$

The output
$f * t =$
$\begin{pmatrix}
  0 & 0 & 0 & 0 & 0\\
  1 & 2 & 3 & 2 & 1\\
  1 & 2 & 3 & 2 & 1\\
  1 & 2 & 3 & 2 & 1\\
  0 & 0 & 0 & 0 & 0\\
\end{pmatrix}$

Pros of this method is that it is a linear operator and reduces noise. The con is that it blurs the image.

\subsubsection{K-nearest Neighbor Averaging}

This method involves averaging the K neighbors that have the "nearest" intesity values. This method is edge preserving.

\subsubsection{Median Filtering}

Median filtering involves taking the average value within a window as the new value.

$f = $
$\begin{pmatrix}
  0 & 0 & 0 & 0 & 0 & 0\\
  0 & 9 & 9 & 9 & 9 & 0\\
  0 & 9 & 9 & 9 & 9 & 0\\
  0 & 9 & 9 & 9 & 9 & 0\\
  0 & 0 & 0 & 0 & 0 & 0\\
\end{pmatrix}$

The output
$f' = $
$\begin{pmatrix}
  0 & 0 & 0 & 0 & 0 & 0\\
  0 & 0 & 9 & 9 & 0 & 0\\
  0 & 9 & 9 & 9 & 9 & 0\\
  0 & 0 & 9 & 9 & 0 & 0\\
  0 & 0 & 0 & 0 & 0 & 0\\
\end{pmatrix}$

A pro of this method is being able to remove noise while keeping the sharpness of the edges. However, this method is nonlinear, and corners are lost. To mitigate the loss of corners, you could use a different sample set, in a cross shape.

\subsubsection{Gaussian Filtering}

Gaussian filtering blurs the image to remove noise. It is still a Gaussian, even after performing multiplication, convolution, and/or Fourier transform. It is a "low-pass" filter that removes unwanted high frequencies. The larger $\sigma$ is, the lower $\sigma_f$ is, which gives a lower cut-off frequency.
\[\sigma \cdot \sigma_f = \frac{1}{2\pi}\]

\subsection{Sharpening}

Sharpening, also known as "deblurring", is where a blurred version of the image is subtracted from the original f. This removes the blurred portions of the image and leaves behind the sharpened bits.

\[\hat{f}(x,y) = f(x,y) - \nabla^{2}f(x,y)=\]
\[5*f(x,y)-[f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)]\]

\[
\begin{bmatrix}
  0 & -1 & 0\\
  -1 & 5 & -1\\
  0 & -1 & 0
\end{bmatrix}
\]

There can be different variations of sharpening, where we apply different weights, \(w\)
\[\hat{f}(x,y) = f(x,y) - w * \nabla^{2}f(x,y)\]

\subsubsection{Laplacian-based Methods}

The Laplacian (2nd derivative) operator is an approximation to the Laplacian $\frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2}$.

$\frac{\partial^2f}{\partial x^2} = f(x+1,y) - f(x,y) - (f(x,y) - f(x-1,y))$

$\frac{\partial^2f}{\partial y^2} = f(x,y+1) - f(x,y) - (f(x,y) - f(x,y-1))$

$\nabla^2 f(x,y) = \frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} = f(x+1,y) + f(x-1,y) + f(x,y+1) + f(x,y-1) - 4 \cdot f(x,y)$

The Laplacian operator is either:
$
\begin{bmatrix}
  0 & 1 & 0\\
  1 & -4 & 1\\
  0 & 1 & 0
\end{bmatrix}
$
or
$
\begin{bmatrix}
  0 & -1 & 0\\
  -1 & 4 & -1\\
  0 & -1 & 0
\end{bmatrix}
$

Some cons of this method is that the directional information is not presented, and it enhances the noise. Zero crossings in the 2nd derivative correspond with edges.

\subsubsection{Unsharp Masking}



\subsection{Image Moments}

The (p,q)$^{th}$ order moment of f(x,y) is:
$m(p,q) = \int \int x^p \cdot y^q \cdot f(x,y) dxdy$
and in discrete form:
$m_{pq} = \Sigma_x \Sigma_y x^p y^q \cdot f(x,y)$.

A segmented image can be represented by a binary image B[i,j] in which "1" could represent, for example, the foreground pixels.

0th order moment: Area A = $\Sigma_i \Sigma_j B[i,j]$
1st order moment: centroid
$\bar x = \frac{\Sigma_i \Sigma_j j \cdot B[i,j]}{A}$,
$\bar y = \frac{\Sigma_i \Sigma_j i \cdot B[i,j]}{A}$

Axis of least 2nd moment is the access of elongation(orientation) for 2D regions.

\section{Template Matching}
% cross correlation vs convolution, how it doesn't matter in cv

Similarity between image $f(\vec p)$ and a template $t(\vec p)$ can be measured by a square of the Euclidean distance:

$d^2(\vec q) = \Sigma_{\vec p}[f(\vec p + \vec q) - t(\vec p)]^2$
where $\Sigma_{\vec p}$ means $\Sigma_{x = -M}^M \Sigma_{y = -N}^N$. If $t(\vec p)$ and $f(\vec p)$ match perfectly, then $d^2(\vec q) = 0$.

Expand it to get:
$d^2(\vec q) = \Sigma_{\vec p}[f^2(\vec p + \vec q) - 2 \cdot f(\vec p + \vec q) \cdot t(\vec p) + t^2(\vec p)]$

Since $f^2(\vec p + \vec q)$ is independent from t, and $t^2(\vec p)$ is a constant, then, to minimize $d^2(\vec q)$, we only need to focus on maximizing $\Sigma_{\vec p} f(\vec p + \vec q) \cdot t(\vec p)$.

Cross Correlation: $R_{ft}(\vec q) = \Sigma_{\vec p} f(\vec p + \vec q) \cdot t(\vec p)$ or $R_{ft}(x, y) = \Sigma_{x = -M}^M \Sigma_{y = -N}^N f(x+x', y+y') \cdot t(x',y')$

The above definition is not invariant to the magnitue of f. A normalized $R_{ft}$ is needed.

$$R_{ft} = \frac{\Sigma_{x'} \Sigma_{y'} f(x+x', y+y') \cdot t(x',y')}{\sqrt{\Sigma_{x'} \Sigma_{y'}(f(x+x', y+y'))^2 \cdot \Sigma_{x'} \Sigma_{y'}(t(x',y'))^2}}$$

\section{Edge Detection}

Edges are intensity discontinuities in images. Edge operators detect the presence of local edges.

\subsection{Gradient-Based Methods}

\subsubsection{Gradient Thresholding}

$(\partial f/\partial x) = f(x+n, y) - f(x,y)$

$(\partial f/\partial y) = f(x, y+n) - f(x,y)$

$s = \sqrt{(\partial f/\partial x)^2 + (\partial f/\partial y)^2}$

$\theta = arctan((\partial f/\partial y)^2/(\partial f/\partial x)^2)$

s is the magnitude, and $\theta$ is the direction.
If the magnitude is greater than a threshold t, then it is an edge, and an edge map can be generated.

\subsubsection{Roberts Operator}

        \[
          \begin{bmatrix}
            0 & 1\\
            -1 & 0
          \end{bmatrix}\quad
          \begin{bmatrix}
            1 & 0\\
            0 & -1
          \end{bmatrix}
        \]
        \[
          \begin{matrix}
            \Delta_1
          \end{matrix}\quad
          \begin{matrix}
            \Delta_2
          \end{matrix}
        \]
\[s = \mid\Delta_1\mid + \mid\Delta_2\mid\]

\subsubsection{Prewitt and Sobel Operators}

Prewitt uses 8 operators

        \[
          \begin{bmatrix}
            -1 & 0 & 1\\
            -1 & 0 & 1\\
            -1 & 0 & 1
          \end{bmatrix}\quad
          \begin{bmatrix}
            0 & 1 & 1\\
            -1 & 0 & 1\\
            -1 & -1 & 0
          \end{bmatrix}
        \]
etc.

(The first one is for 0$^{\circ}$ and the second one is for 45$^{\circ}$)

Pros of this method include providing directional information and using local averages to reduce noise. However, the edge width is greater than 1, which means there isn't a single input.

Sobel is similar to Prewitt, and also uses 8 operators.

        \[
          \begin{bmatrix}
            -1 & 0 & 1\\
            -2 & 0 & 2\\
            -1 & 0 & 1
          \end{bmatrix}\quad
          \begin{bmatrix}
            0 & 1 & 2\\
            -1 & 0 & 1\\
            -2 & -1 & 0
          \end{bmatrix}
        \]
etc.

(The first one is for 0$^{\circ}$ and the second one is for 45$^{\circ}$)

\subsection{Laplacian-Based Methods}
% log vs dog

\begin{itemize}
  \item {LOG (Laplacian of Gaussian)}
  
  Mark zero crossings (where the values are zero)
  \item {DOG (Difference of Gaussians)}
  
  Can change $\sigma$ of a Gaussian to find edges with different spatial frequencies
  LOG can be approximated with DOG
\end{itemize}

\subsection{Marr's Edge Operator}

Given an image f(x,y), edges can be found by locating the zero crossings from \(D^2(G(x,y)*f(x,y))\), where G is a Gaussian function and $D^2$ is the second derivative.
Derivative rule for convolution: \(D^2(G*f) = D^2G * f\).

\subsection{Canny Edge Operator}

\subsubsection{Criteria for Good Edge Detection}
\begin{itemize}
      \item Always find real edges, not false ones
  
      \item Good localization, where found edges are as close to the original as possible
  
      \item Unique Output
\end{itemize}

% criterion for good edge detection
% 5 stages of Canny Edge Operator
\begin{enumerate}
        \item \textbf{Smoothing}
        
        Use Gaussian I * G, where I is the image matrix and G is the Gaussian mask
        \item \textbf{Gradient Operator}
        
        Use the horizontal and vertical Sobel operators to generate the x and y derivatives of the image function.
        By applying filters from both the horizontal and vertical directions, the gradient calculation can accurately determine the intensity of edges.

        \[
          \begin{bmatrix}
            -1 & 0 & 1\\
            -2 & 0 & 2\\
            -1 & 0 & 1
          \end{bmatrix}\quad
          \begin{bmatrix}
            1 & 2 & 1\\
            0 & 0 & 0\\
            -1 & -2 & -1
          \end{bmatrix}
        \]

        Use the derivatives to determine the magnitude and $\theta$ of the image.
        
        $s = \sqrt{I_x^2 + I_y^2}$

        $I_x$ and $I_y$ are the images after applying the Sobel filters, and s is the magnitude.

        $\theta = arctan(I_y/I_x)$

        \item \textbf{Non-maximum Suppression}
        
        Non-maximum suppression thins out the edges that result from the gradient operator.
        The algorithm goes through all the elements of the gradient intensity matrix and finds pixels with the maximum value in the edge directions.

        \item \textbf{Double Thresholding}
        
        Double thresholding determines the strength of each pixel, whether it's weak, strong, or irrelevant.
        There are two thresholds, a high threshold to determine the strong pixels and a low threshold to filter out the irrelevant pixels.
        Strong pixels are included in the final image, and weak pixels have to be filtered through again to see whether or not they are strong or discarded.
        
        \item \textbf{Hysteresis Tracking}
        
        This is used to determine if weak pixels are going to be transformed into strong pixels and included in the final edge map.
        If a weak pixel has at least one pixel surrounding it that is strong, then it is turned into a strong pixel.
\end{enumerate}

\section{Regions and Segmentation}

A region is a group of connected pixels with similar properties in an image.
\subsection{Image Segmentation}

Image segmentation is where the image is divided into regions. It can be region-based, edge-based, or a combination of both.
Different Methods of Image Segmentation:
\begin{enumerate}
  \item {Thresholding}
  \item {Blob colouring (for binary images)}
  \item {Split and Merge}
  \item {K-Means}
  \item {Mean Shift}
\end{enumerate}

\subsection{Gestalt Psychology and Human Perception}

Grouping is the key to visual perception. 

\begin{itemize}
  \item {Proximity}
  \item {Similarity}
  \item {Common Fate}
  
  Items that have similar motion.
  \item {Common Region}
  \item {Parallelism}
  \item {Symmetry}
  \item {Continuity}
  \item {Closure}
  \item {Familiar Shapes}
\end{itemize}

An image can be separated into a figure and a ground.

\section{Texture Analysis}

Texture is repeated patterns of local intensity variations, and it depends on the image resolution.
There are three issues related to texture analysis:

\begin{itemize}
  \item {texture classification}
  \item {texture segmentation}
  \item {shape from texture}
\end{itemize}

\subsection{Statistical Methods}

\subsubsection{Spatial Gray Level Dependence Method}

Define a gray level co-occurrence matrix. A gray level co-occurrence matrix is a matrix that contains information about the positions of pixels having similar gray level values.

  \[
    \begin{bmatrix}
      2 & 1 & 2 & 0 & 1\\
      0 & 2 & 1 & 1 & 2\\
      0 & 1 & 2 & 2 & 0\\
      1 & 2 & 2 & 0 & 1\\
      2 & 0 & 1 & 0 & 1
    \end{bmatrix}
  \]

This is the matrix of a 5 x 5 image with 3 gray levels.

1/16 * 
\(
  \begin{bmatrix}
    0 & 2 & 2\\
    2 & 1 & 2\\
    2 & 3 & 2
  \end{bmatrix}
  \)
  
  This is the gray level co-occurrence matrix for d = (1,1). The indices i are for the rows (0,1,2), and j are for the columns (0,1,2). This array is called P(i,j).
  The algorithm for constructing a gray level co-occurrence matrix is counting all the pairs of pixels in which the first pixel has value i, and its matching pair displaced from first pixel by d has a value of j.

  \[Entropy = -\Sigma_i \Sigma_j P[i,j]logP[i,j]\]

  Entropy is the level of randomness in the data that is being processed. Entropy is highest when all the elements in P[i,j] are the same. In terms of images, this means that there is no preferred gray level.
  
  \[Energy = \Sigma_i \Sigma_j P^2[i,j]\]

  \[Contrast = \Sigma_i \Sigma_j (i-j)^2 P[i,j]\]

  Contrast is a measure of the local variations in an image. If there is a large amount of variation,
  the P[i,j]'s will be concentrated away from the main diagonal and the contrast will be high.

  \[Homogeneity = \Sigma_i \Sigma_j \!\frac{P[i,j]}{1+\mid i-j \mid}\]

  Homogenous images result in a co-occurrence matrix with a combination of high and low P[i,j]'s. If the range of gray levels is small, then P[i,j] will be concentrated along the main diagonal.

  P[i,j] is not symmetric, since the number of pixels having gray levels [i,j] does not necessarily equal the number of pixels with gray levels [j,i].
  Elements of P[i,j] is normalized by 1/16, where 16 is the number of pixel pairs.
  The normalized P[i,j] is treated as a probability mass function because $\Sigma_i,j P[i,j] = 1$.

  Negatives of this method is that many d's have to be tried before arriving at a good one, and it's not good for textures that are composed of primitives of a certain shape/structure.

\subsubsection{Gray Level Run Length Method}
% 5 texture measures
Define gray level run matrix $P_\theta$, where $P_\theta(i,j)$ is the number of times an image contains a run of length j for pixels with value i in a certain direction of $\theta$.
Long runs can be expected for images with coarse texture.

\[T_\theta = \Sigma_{i=1} ^{N_g} \Sigma_{j=1} ^{N_r} P_\theta(i,j)\]
where $N_g$ is the number of gray levels and $N_r$ is the number of run lengths

\begin{enumerate}
  \item {Short run emphasis inverse moments}
  
  \[\frac{1}{T_\theta} \cdot \Sigma_{i=1} ^{N_g} \Sigma_{j=1} ^{N_r} \frac{P_\theta (i,j)}{j^2}\]

  This measure emphasizes short runs of a gray level image. Short run emphasis is large when there are lots of short runs of the same intensity.

  \item {Long run emphasis moments}
  
  \[\frac{1}{T_\theta} \cdot \Sigma_{i=1} ^{N_g} \Sigma_{j=1} ^{N_r} j^2 \cdot P_\theta (i,j)\]

  This measure emphasizes long runs of a gray level image. Long run emphasis is large when there are lots of long runs of the same intensity.

  \item {Gray level nonuniformity}
  
  \[\frac{1}{T_\theta} \cdot \Sigma_{i=1} ^{N_g} (\Sigma_{j=1} ^{N_r} P_\theta (i,j))^2\]

  The sum inside of the brackets is the total number of runs for a certain gray level value. This value is large if the runs are not evenly distributed across different intensities.

  \item {Run length nonuniformity}
  
  \[\frac{1}{T_\theta} \cdot \Sigma_{j=1} ^{N_r} (\Sigma_{i=1} ^{N_g} P_\theta (i,j))^2\]

  The sum inside the brackets is the total number of runs with a certain run length at a certain gray level.

  \item {Inverse of weighted average run length}
  
\[\frac{T_\theta}{\Sigma_{i=1} ^{N_g} \Sigma_{j=1} ^{N_r} j \cdot P_\theta (i,j)}\]

This measure puts more weight into longer run length.

\end{enumerate}

Use these 5 measures in each of the 4 directions. For these textures (swamp, lake, railroad, orchard, scrub, suburb), the measures have 83$\%$ accuracy.

\subsection{Structural Methods}

Textures are composed of primitives known as "texels".

\begin{itemize}
  \item {Works better than statistical methods if the primitives can be detected}
  \item {Geometric properties (size, elongation, orientation, etc.) of the primitives are important features}
  \item {Spatial relationship of the primitives based on the co-occurrence of these primitives can be further explored}
\end{itemize}

For example, if we use an edge-based method, then the primitives are edges, the properties are orientation, gradient, and density, and the spatial relationship is edge separations.

\subsubsection{Tamura's Texture Measures}

\begin{itemize}
  \item {Coarseness}
  \item {Contrast}
  \item {Directionality}
\end{itemize}

 \section{Classical Hough Transform}

 \section{Generalized Hough Transform}
 \subsection{R-Table Generation}

 \section{Representations of 2D Geometric Structures}
 \subsection{Boundary Representations}
 \subsubsection{Polyline}

  Polyline can be used to apporximate curves and contours accurately.
  
  The algorithm for curve approximation using polyline involves iteratively going through vertices, and adding new vertices if the maximum distance between the polyline and the shape is greater than $T_{max}$.

  For closed polylines, such as polygon, the area is $\frac{1}{2}\Sigma_{i=0}^{n-1}(x_{i+1}y_i + x_iy_{i+1})$.

 \subsubsection{Chain Code}

  Chain code is an economic coding method that has either 4 or 8 neighbours.
  The derivative of the chain code is equivalent to the difference of the chain code modulo 4 (or 8).
  A pro of this method is that it is rotation invariant.

  For the area of an enclosed space:
  For each element of the chain code in counterclockwise order, there are 4 cases
  
  \vspace{2mm}
  
  Case 0: area = area - y

  \vspace{2mm}

  Case 1: y = y + 1

  \vspace{2mm}

  Case 2: area = area + y

  \vspace{2mm}

  Case 3: y = y - 1

 \subsubsection{Curvature Scale Space (CSS)}

  Find the zero crossings of the image's contour (inflection points).
  The positions of these inflection points are relative to the length of the contour curve.
  The boundary function is convolved with a Gaussian kernel of increasing width.
  At each $\sigma$, the curvature function is computed and the zero crossing points are identified and plotted.
  As $\sigma$ increases, the contour will lose its inflection points and become a convex blob.
  
  \vspace{5mm}
  
  When $\sigma$ increases, no new inflection points are created, but their locations are shifted.
  More prominent structures in the contour can survive large values of $\sigma$.
  The localization problem of inflection points can be solved by tracking their locations in the scale-space image back to the original.
  It is a robust method, that is also fast and compact.
  CSS can be used for shape comparison, finding similarities and differences between images.

  \vspace{5mm}

  In a shape context descriptor, the image is separated into several bins, and you count the number of points that fall within each bin. This displays a distribution of points relative to each point.
  Solve for least cost assignments, $C_{ij}$ to get correspondences. And then use a template given by the correspondences.

 \subsection{Region Representations}
 \subsubsection{Spatial Occupancy Array}

  This method is simple and results in a sparse array.

 \subsubsection{Axis-based Representations}

  This method involves putting images on a grid and figuring out which x values correspond with a specific y value of the image (vice versa for x-axis).
  A con of this method is that it usually requires a combination of both X- and Y-axis representations.

 \subsubsection{Quad-trees}
 % split and merge

  A pro of this method is that it is a hierarchical tree representation. A con is that the representation depends on the chosen grid size. Also objects can't be easily shifted or scaled.
  
  \vspace{5mm}

  Algorithm: Split and Merge

  \vspace{5mm}

  This algorithm uses a quad tree for region growing. You start at an intermediate level, such as 64 x 64, then do a bottom up process (merge), and a top down process (split).
  During quad-tree construction, pixels in the pyramid are either black or white.
  You intersect two quad trees R1 and R2, and do the following steps:
  If R1 is black, then return R2. If R1 is white, return white. If R2 is black, return R1. If R2 is white, return white.
  Next, you determine the elements of the new tree, $R_{new}$.
  The NW element of $R_{new}$ is the intersect between NW R1 and NW R2.
  The NE element of $R_{new}$ is the intersect between NE R1 and NE R2.
  The SW element of $R_{new}$ is the intersect between SW R1 and SW R2.
  The SE element of $R_{new}$ is the intersect between SE R1 and SE R2.

 \subsubsection{Medial Axis Transform (MAT)}

 % continue from here from "distances"

  MAT is a thinning algorithm to get the skeleton of the region.
  For each point p, find the closest neighbors on the region boundary.


\subsection{Distances}
\subsection{Feature Space}
\subsection{K-means Clustering}
\subsection{Mean Shift Algorithm}
\subsection{Gaussian and Laplacian Pyramids}

\section{Representations of 3D Structures}
\subsection{Surface Representations}
\subsubsection{Polyhedral Surfaces}
\subsubsection{Generalized Cylinders}
\subsection{Volumetric Representations}
\subsubsection{Spatial Occupancy Array}
\subsubsection{Octree}
\subsubsection{Constructive Solid Geometry}
\subsubsection{Shape Histogram}
\subsection{Viewer-Centered Representation}
\subsection{Object-Centered Representation}

\end{multicols}

\end{document}
